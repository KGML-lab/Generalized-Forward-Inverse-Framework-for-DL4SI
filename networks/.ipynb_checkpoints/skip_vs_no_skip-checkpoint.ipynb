{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"(convolution => [GroupNorm] => LeakyReLU) * 2\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.GroupNorm(1, out_channels, 1e-3),\n",
    "            nn.LeakyReLU(negative_slope=0.01, inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.GroupNorm(1, out_channels, 1e-3),\n",
    "            nn.LeakyReLU(negative_slope=0.01, inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "class Down(nn.Module):\n",
    "    \"\"\"Downscaling with maxpool then double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            DoubleConv(in_channels, out_channels),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.maxpool_conv(x)\n",
    "    \n",
    "class Up(nn.Module):\n",
    "    \"\"\"Upscaling then double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
    "        super().__init__()\n",
    "\n",
    "        # if bilinear, use the normal convolutions to reduce the number of channels\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(in_channels // 2, in_channels // 2, kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv = DoubleConv(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x1, x2, skip=True):\n",
    "        x1 = self.up(x1)\n",
    "        \n",
    "        # input is CHW\n",
    "        diffY = x2.size()[2] - x1.size()[2]\n",
    "        diffX = x2.size()[3] - x1.size()[3]\n",
    "\n",
    "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
    "                        diffY // 2, diffY - diffY // 2])\n",
    "        \n",
    "        if skip:\n",
    "            x = torch.cat([x2, x1], dim=1)\n",
    "        else:\n",
    "            x = x1\n",
    "        return self.conv(x)\n",
    "    \n",
    "class RepeatDownBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, repeat=1):\n",
    "        super().__init__()\n",
    "        self.block = [Down(in_channels, out_channels)]\n",
    "        for i in range(0, repeat-1):\n",
    "            self.block.append(DoubleConv(out_channels, out_channels))\n",
    "        self.block = nn.ModuleList(self.block)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in self.block:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "    \n",
    "class RepeatBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, repeat=1):\n",
    "        super().__init__()\n",
    "        self.block = []\n",
    "        for i in range(0, repeat):\n",
    "            self.block.append(DoubleConv(out_channels, out_channels))\n",
    "        self.block = nn.ModuleList(self.block)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in self.block:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "class RepeatUpBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, repeat=1):\n",
    "        super().__init__()\n",
    "        self.block = [Up(in_channels, out_channels)]\n",
    "        for i in range(0, repeat-1):\n",
    "            self.block.append(DoubleConv(out_channels, out_channels))\n",
    "        self.block = nn.ModuleList(self.block)\n",
    "    \n",
    "    def forward(self, x1, x2, skip=True):\n",
    "        x = self.block[0](x1, x2, skip)\n",
    "        for layer in self.block[1:]:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels, d, repeat=4, skip=True):\n",
    "        super(UNet, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.d = d\n",
    "        self.skip = skip\n",
    "        \n",
    "        # Encoder path\n",
    "        self.encoder = []\n",
    "        for depth in range(0, d):\n",
    "            in_dim = 2**depth * self.in_channels\n",
    "            self.encoder.append(RepeatDownBlock(in_dim, 2*in_dim, repeat=repeat))\n",
    "        self.encoder = nn.ModuleList(self.encoder)\n",
    "        \n",
    "        bottleneck_dim = 2*in_dim\n",
    "        self.bottleneck = RepeatBlock(bottleneck_dim, bottleneck_dim, repeat=2*repeat)\n",
    "\n",
    "        # Decoder path\n",
    "        self.decoder = []\n",
    "        for depth in range(d-1, -1, -1):\n",
    "            in_dim = 2**(depth+1) * self.in_channels\n",
    "            in_ch_ = 2*in_dim if skip else in_dim\n",
    "            self.decoder.append(RepeatUpBlock(in_ch_, in_dim//2, repeat=repeat)) #due to skip connections\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        self.decoder.append(DoubleConv(self.in_channels, self.in_channels))\n",
    "        self.decoder = nn.ModuleList(self.decoder)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        enc_outputs = []\n",
    "        for layer in self.encoder:\n",
    "            x = layer(x)\n",
    "            enc_outputs.append(x)\n",
    "        x = self.bottleneck(x)\n",
    "        for i, layer in enumerate(self.decoder[:-1]):\n",
    "            x = layer(x, enc_outputs[-(i+1)], skip=self.skip)\n",
    "        x = self.upsample(x)\n",
    "        x = self.decoder[-1](x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model, verbose=True):\n",
    "    num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    if num_params >= 1e6:\n",
    "        num_params /= 1e6\n",
    "        suffix = \"M\"\n",
    "    elif num_params >= 1e3:\n",
    "        num_params /= 1e3\n",
    "        suffix = \"K\"\n",
    "    else:\n",
    "        suffix = \"\"\n",
    "    if verbose:\n",
    "        print(f\"Number of trainable parameters: {num_params:.2f}{suffix}\")\n",
    "    return num_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 128, 70, 70])\n"
     ]
    }
   ],
   "source": [
    "model = UNet(in_channels=128, d=2, repeat=2, skip=True)\n",
    "inputs = torch.randn(10, 128, 70, 70)\n",
    "out = model(inputs)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters: 34.68M\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "34.67904"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 128, 70, 70])\n"
     ]
    }
   ],
   "source": [
    "model = UNet(in_channels=128, d=2, repeat=2, skip=False)\n",
    "inputs = torch.randn(10, 128, 70, 70)\n",
    "out = model(inputs)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters: 33.20M\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "33.20448"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class VelAutoEncoder(nn.Module):\n",
    "    def __init__(self, input_channel, encoder_channels, decoder_channels):\n",
    "        super(VelAutoEncoder, self).__init__()\n",
    "        \n",
    "        encoder_layers = nn.ModuleList()\n",
    "        decoder_layers = nn.ModuleList()\n",
    "        \n",
    "        enlayer1 = nn.Sequential(nn.Conv2d(input_channel, encoder_channels[0],\n",
    "                                                kernel_size=1, stride=1, padding=0, \n",
    "                                                padding_mode='reflect'),\n",
    "                                     nn.BatchNorm2d(encoder_channels[0]),\n",
    "                                     nn.LeakyReLU(0.2, inplace=True))\n",
    "        encoder_layers.append(enlayer1)\n",
    "        \n",
    "        enlayer2 = nn.Sequential(nn.Conv2d(encoder_channels[0], encoder_channels[1],\n",
    "                                                kernel_size=1, stride=1, padding=0,\n",
    "                                                padding_mode='reflect'),\n",
    "                                     nn.BatchNorm2d(encoder_channels[1]),\n",
    "                                     nn.LeakyReLU(0.2, inplace=True))\n",
    "        \n",
    "        encoder_layers.append(enlayer2)\n",
    "        \n",
    "        enlayer3 = nn.Sequential(nn.Conv2d( encoder_channels[1], encoder_channels[2],\n",
    "                                                kernel_size=(3, 3), stride=1, padding=0,\n",
    "                                                padding_mode='reflect'),\n",
    "                                     nn.BatchNorm2d(encoder_channels[2]),\n",
    "                                     nn.LeakyReLU(0.2, inplace=True))\n",
    "        \n",
    "        encoder_layers.append(enlayer3)\n",
    "        \n",
    "        enlayer4 = nn.Sequential(nn.Conv2d(encoder_channels[2], encoder_channels[3],\n",
    "                                                kernel_size=(3, 3), stride=(2,2), padding=(1,1),\n",
    "                                                padding_mode='reflect'),\n",
    "                                     nn.BatchNorm2d(encoder_channels[3]),\n",
    "                                     nn.LeakyReLU(0.2, inplace=True))\n",
    "        \n",
    "        encoder_layers.append(enlayer4)\n",
    "        \n",
    "        enlayer5 = nn.Sequential(nn.Conv2d(encoder_channels[3], encoder_channels[4],\n",
    "                                                kernel_size=(3, 3), stride=(2,2), padding=0,\n",
    "                                                padding_mode='reflect'),\n",
    "                                     nn.BatchNorm2d(encoder_channels[4]),\n",
    "                                     nn.LeakyReLU(0.2, inplace=True))\n",
    "        \n",
    "        encoder_layers.append(enlayer5)\n",
    "        \n",
    "        enlayer5 = nn.Sequential(nn.Conv2d(encoder_channels[4], encoder_channels[5],\n",
    "                                                kernel_size=(3, 3), stride=1, padding=(1,1),\n",
    "                                                padding_mode='reflect'),\n",
    "                                     nn.BatchNorm2d(encoder_channels[5]),\n",
    "                                     nn.LeakyReLU(0.2, inplace=True))\n",
    "        \n",
    "        encoder_layers.append(enlayer5)\n",
    "        \n",
    "        enlayer6 = nn.Upsample((16,16), mode='bilinear')\n",
    "        \n",
    "        encoder_layers.append(enlayer6)\n",
    "        \n",
    "        \n",
    "        delayer1 = nn.Sequential(nn.ConvTranspose2d(encoder_channels[-1], decoder_channels[0],\n",
    "                                                kernel_size=(3,3), stride=1, padding=0,padding_mode='zeros'),\n",
    "                             nn.BatchNorm2d(decoder_channels[0]),\n",
    "                             nn.Tanh())\n",
    "        \n",
    "        decoder_layers.append(delayer1)\n",
    "        \n",
    "        delayer2 = nn.Sequential(nn.ConvTranspose2d(decoder_channels[0], decoder_channels[1],\n",
    "                                                kernel_size=(3,3), stride=(2,2), padding=0,padding_mode='zeros'),\n",
    "                             nn.BatchNorm2d(decoder_channels[1]),\n",
    "                             nn.Tanh())\n",
    "        \n",
    "        decoder_layers.append(delayer2)\n",
    "        \n",
    "        delayer3 = nn.Sequential(nn.ConvTranspose2d(decoder_channels[1], decoder_channels[2],\n",
    "                                                kernel_size=(3,3), stride=1, padding=0,padding_mode='zeros'),\n",
    "                             nn.BatchNorm2d(decoder_channels[2]),\n",
    "                             nn.Tanh())\n",
    "        \n",
    "        decoder_layers.append(delayer3)\n",
    "        \n",
    "        delayer4 = nn.Sequential(nn.ConvTranspose2d(decoder_channels[2], decoder_channels[3],\n",
    "                                                kernel_size=1, stride=1, padding=0,padding_mode='zeros'),\n",
    "                             nn.BatchNorm2d(decoder_channels[3]),\n",
    "                             nn.Tanh())\n",
    "        \n",
    "        decoder_layers.append(delayer4)\n",
    "        \n",
    "        delayer5 = nn.Sequential(nn.ConvTranspose2d(decoder_channels[3], decoder_channels[4],\n",
    "                                                kernel_size=1, stride=1, padding=0,padding_mode='zeros'),\n",
    "                             nn.BatchNorm2d(decoder_channels[4]),\n",
    "                             nn.Tanh(),\n",
    "                             nn.Conv2d(decoder_channels[4], decoder_channels[4], kernel_size=1, stride=1)\n",
    "                             )\n",
    "        decoder_layers.append(delayer5)\n",
    "        \n",
    "        delayer5 = nn.Upsample((70,70), mode='bilinear')\n",
    "        \n",
    "        decoder_layers.append(delayer5)\n",
    "        \n",
    "        self.encoder_layers = encoder_layers\n",
    "        self.decoder_layers = decoder_layers\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.encoder_layers(x)\n",
    "        x = self.decoder_layers(x)\n",
    "        return x\n",
    "    \n",
    "    def embedding(self, x):\n",
    "        for layer in self.encoder_layers:\n",
    "            x = layer(x)\n",
    "            print(x.shape)\n",
    "        return x\n",
    "    \n",
    "    def decoder(self, x):\n",
    "        for layer in self.decoder_layers:\n",
    "            x = layer(x)\n",
    "            print(x.shape)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae = VelAutoEncoder(input_channel=1, encoder_channels=[8, 16, 32, 64, 128, 256], decoder_channels=[128, 64, 32, 16, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 8, 70, 70])\n",
      "torch.Size([10, 16, 70, 70])\n",
      "torch.Size([10, 32, 68, 68])\n",
      "torch.Size([10, 64, 34, 34])\n",
      "torch.Size([10, 128, 16, 16])\n",
      "torch.Size([10, 256, 16, 16])\n",
      "torch.Size([10, 256, 16, 16])\n"
     ]
    }
   ],
   "source": [
    "inputs = torch.randn(10, 1, 70, 70)\n",
    "out = ae.embedding(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 128, 34, 34])\n",
      "torch.Size([10, 64, 69, 69])\n",
      "torch.Size([10, 32, 71, 71])\n",
      "torch.Size([10, 16, 71, 71])\n",
      "torch.Size([10, 1, 71, 71])\n",
      "torch.Size([10, 1, 70, 70])\n"
     ]
    }
   ],
   "source": [
    "inputs = torch.randn(10, 128, 32, 32)\n",
    "out = ae.decoder(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_dict = { \n",
    "                  'in_dim':(70, 70),\n",
    "                  'latent_dim':(64,64),\n",
    "                  'in_channels':1,\n",
    "                  'encoder_blocks':{\n",
    "                       \"0\": {\n",
    "                            \"out_channels\":8,\n",
    "                            \"kernel_size\":1, \n",
    "                            \"stride\":1,\n",
    "                            \"padding\":0,\n",
    "                            \"activation\":{\n",
    "                                \"activation_fn\":\"nn.LeakyReLU\",\n",
    "                                \"activation_params\":{\n",
    "                                    \"negative_slope\":0.2,\n",
    "                                    \"inplace\":True,\n",
    "                                },\n",
    "                            },\n",
    "                            \"padding_mode\":'reflect',\n",
    "                           },\n",
    "                        \"1\": {\n",
    "                            \"out_channels\":16,\n",
    "                            \"kernel_size\":1, \n",
    "                            \"stride\":1,\n",
    "                            \"padding\":0,\n",
    "                            \"activation\":{\n",
    "                                \"activation_fn\":\"nn.LeakyReLU\",\n",
    "                                \"activation_params\":{\n",
    "                                    \"negative_slope\":0.2,\n",
    "                                    \"inplace\":True,\n",
    "                                },\n",
    "                            },\n",
    "                            \"padding_mode\":'reflect',\n",
    "                           },\n",
    "                      \"2\": {\n",
    "                            \"out_channels\":32,\n",
    "                            \"kernel_size\":(3,3), \n",
    "                            \"stride\":1,\n",
    "                            \"padding\":0,\n",
    "                            \"activation\":{\n",
    "                                \"activation_fn\":\"nn.LeakyReLU\",\n",
    "                                \"activation_params\":{\n",
    "                                    \"negative_slope\":0.2,\n",
    "                                    \"inplace\":True,\n",
    "                                },\n",
    "                            },\n",
    "                            \"padding_mode\":'reflect',\n",
    "                           },\n",
    "                      \"3\": {\n",
    "                            \"out_channels\":64,\n",
    "                            \"kernel_size\":(3,3), \n",
    "                            \"stride\":(2,2),\n",
    "                            \"padding\":(1,1),\n",
    "                            \"activation\":{\n",
    "                                \"activation_fn\":\"nn.LeakyReLU\",\n",
    "                                \"activation_params\":{\n",
    "                                    \"negative_slope\":0.2,\n",
    "                                    \"inplace\":True,\n",
    "                                },\n",
    "                            },\n",
    "                            \"padding_mode\":'reflect',\n",
    "                           },\n",
    "                      \"4\": {\n",
    "                            \"out_channels\":128,\n",
    "                            \"kernel_size\":(3,3), \n",
    "                            \"stride\":(2,2),\n",
    "                            \"padding\":0,\n",
    "                            \"activation\":{\n",
    "                                \"activation_fn\":\"nn.LeakyReLU\",\n",
    "                                \"activation_params\":{\n",
    "                                    \"negative_slope\":0.2,\n",
    "                                    \"inplace\":True,\n",
    "                                },\n",
    "                            },\n",
    "                            \"padding_mode\":'reflect',\n",
    "                           },\n",
    "                        \"5\": {\n",
    "                            \"out_channels\":256,\n",
    "                            \"kernel_size\":(3,3), \n",
    "                            \"stride\":1,\n",
    "                            \"padding\":(1,1),\n",
    "                            \"activation\":{\n",
    "                                \"activation_fn\":\"nn.LeakyReLU\",\n",
    "                                \"activation_params\":{\n",
    "                                    \"negative_slope\":0.2,\n",
    "                                    \"inplace\":True,\n",
    "                                },\n",
    "                            },\n",
    "                            \"padding_mode\":'reflect',\n",
    "                           },\n",
    "                      },\n",
    "                  'decoder_blocks': {\n",
    "                      \"0\": {\n",
    "                            \"out_channels\":128,\n",
    "                            \"kernel_size\":(3,3), \n",
    "                            \"stride\":1,\n",
    "                            \"padding\":0,\n",
    "                            \"activation\":{\n",
    "                                \"activation_fn\":\"nn.Tanh\",\n",
    "                                \"activation_params\":{\n",
    "                                },\n",
    "                            },\n",
    "                            \"padding_mode\":'zeros',\n",
    "                           },\n",
    "                        \"1\": {\n",
    "                            \"out_channels\":64,\n",
    "                            \"kernel_size\":(3,3), \n",
    "                            \"stride\":(2,2),\n",
    "                            \"padding\":0,\n",
    "                            \"activation\":{\n",
    "                                \"activation_fn\":\"nn.Tanh\",\n",
    "                                \"activation_params\":{\n",
    "                                },\n",
    "                            },\n",
    "                            \"padding_mode\":'zeros',\n",
    "                           },\n",
    "                      \"2\": {\n",
    "                            \"out_channels\":32,\n",
    "                            \"kernel_size\":(3,3), \n",
    "                            \"stride\":1,\n",
    "                            \"padding\":0,\n",
    "                            \"activation\":{\n",
    "                                \"activation_fn\":\"nn.Tanh\",\n",
    "                                \"activation_params\":{\n",
    "                                },\n",
    "                            },\n",
    "                            \"padding_mode\":'zeros',\n",
    "                           },\n",
    "                      \"3\": {\n",
    "                            \"out_channels\":16,\n",
    "                            \"kernel_size\":1, \n",
    "                            \"stride\":1,\n",
    "                            \"padding\":0,\n",
    "                            \"activation\":{\n",
    "                                \"activation_fn\":\"nn.Tanh\",\n",
    "                                \"activation_params\":{\n",
    "                                },\n",
    "                            },\n",
    "                            \"padding_mode\":'zeros',\n",
    "                           },\n",
    "                      \"4\": {\n",
    "                            \"out_channels\":1,\n",
    "                            \"kernel_size\":1, \n",
    "                            \"stride\":1,\n",
    "                            \"padding\":0,\n",
    "                            \"activation\":{\n",
    "                                \"activation_fn\":\"nn.Tanh\",\n",
    "                                \"activation_params\":{\n",
    "                                },\n",
    "                            },\n",
    "                            \"padding_mode\":'zeros',\n",
    "                           },\n",
    "                  },\n",
    "                  \"last_conv2d\":{\n",
    "                      \"kernel_size\":1, \n",
    "                      \"stride\":1,\n",
    "                      \"padding\":0,\n",
    "                    }\n",
    "                }\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('../configs/velocity_config_latent_dim_32.json', 'w') as file:\n",
    "    json.dump(params_dict, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "vel_params_70 = { \n",
    "                  'in_dim':(70, 70),\n",
    "                  'latent_dim':(70,70),\n",
    "                  'in_channels':1,\n",
    "                  'encoder_blocks':{\n",
    "                       \"0\": {\n",
    "                            \"out_channels\":8,\n",
    "                            \"kernel_size\":1, \n",
    "                            \"stride\":1,\n",
    "                            \"padding\":0,\n",
    "                            \"activation\":{\n",
    "                                \"activation_fn\":\"nn.LeakyReLU\",\n",
    "                                \"activation_params\":{\n",
    "                                    \"negative_slope\":0.2,\n",
    "                                    \"inplace\":True,\n",
    "                                },\n",
    "                            },\n",
    "                            \"padding_mode\":'reflect',\n",
    "                           },\n",
    "                        \"1\": {\n",
    "                            \"out_channels\":16,\n",
    "                            \"kernel_size\":1, \n",
    "                            \"stride\":1,\n",
    "                            \"padding\":0,\n",
    "                            \"activation\":{\n",
    "                                \"activation_fn\":\"nn.LeakyReLU\",\n",
    "                                \"activation_params\":{\n",
    "                                    \"negative_slope\":0.2,\n",
    "                                    \"inplace\":True,\n",
    "                                },\n",
    "                            },\n",
    "                            \"padding_mode\":'reflect',\n",
    "                           },\n",
    "                      \"2\": {\n",
    "                            \"out_channels\":32,\n",
    "                            \"kernel_size\":1, \n",
    "                            \"stride\":1,\n",
    "                            \"padding\":0,\n",
    "                            \"activation\":{\n",
    "                                \"activation_fn\":\"nn.LeakyReLU\",\n",
    "                                \"activation_params\":{\n",
    "                                    \"negative_slope\":0.2,\n",
    "                                    \"inplace\":True,\n",
    "                                },\n",
    "                            },\n",
    "                            \"padding_mode\":'reflect',\n",
    "                           },\n",
    "                      \"3\": {\n",
    "                            \"out_channels\":64,\n",
    "                            \"kernel_size\":1, \n",
    "                            \"stride\":1,\n",
    "                            \"padding\":0,\n",
    "                            \"activation\":{\n",
    "                                \"activation_fn\":\"nn.LeakyReLU\",\n",
    "                                \"activation_params\":{\n",
    "                                    \"negative_slope\":0.2,\n",
    "                                    \"inplace\":True,\n",
    "                                },\n",
    "                            },\n",
    "                            \"padding_mode\":'reflect',\n",
    "                           },\n",
    "                      \"4\": {\n",
    "                            \"out_channels\":128,\n",
    "                            \"kernel_size\":1, \n",
    "                            \"stride\":1,\n",
    "                            \"padding\":0,\n",
    "                            \"activation\":{\n",
    "                                \"activation_fn\":\"nn.LeakyReLU\",\n",
    "                                \"activation_params\":{\n",
    "                                    \"negative_slope\":0.2,\n",
    "                                    \"inplace\":True,\n",
    "                                },\n",
    "                            },\n",
    "                            \"padding_mode\":'reflect',\n",
    "                           },\n",
    "                      },\n",
    "                  'decoder_blocks': {\n",
    "                      \"0\": {\n",
    "                            \"out_channels\":128,\n",
    "                            \"kernel_size\":1, \n",
    "                            \"stride\":1,\n",
    "                            \"padding\":0,\n",
    "                            \"activation\":{\n",
    "                                \"activation_fn\":\"nn.Tanh\",\n",
    "                                \"activation_params\":{\n",
    "                                },\n",
    "                            },\n",
    "                            \"padding_mode\":'zeros',\n",
    "                           },\n",
    "                        \"1\": {\n",
    "                            \"out_channels\":64,\n",
    "                            \"kernel_size\":1, \n",
    "                            \"stride\":1,\n",
    "                            \"padding\":0,\n",
    "                            \"activation\":{\n",
    "                                \"activation_fn\":\"nn.Tanh\",\n",
    "                                \"activation_params\":{\n",
    "                                },\n",
    "                            },\n",
    "                            \"padding_mode\":'zeros',\n",
    "                           },\n",
    "                      \"2\": {\n",
    "                            \"out_channels\":32,\n",
    "                            \"kernel_size\":1, \n",
    "                            \"stride\":1,\n",
    "                            \"padding\":0,\n",
    "                            \"activation\":{\n",
    "                                \"activation_fn\":\"nn.Tanh\",\n",
    "                                \"activation_params\":{\n",
    "                                },\n",
    "                            },\n",
    "                            \"padding_mode\":'zeros',\n",
    "                           },\n",
    "                      \"3\": {\n",
    "                            \"out_channels\":16,\n",
    "                            \"kernel_size\":1, \n",
    "                            \"stride\":1,\n",
    "                            \"padding\":0,\n",
    "                            \"activation\":{\n",
    "                                \"activation_fn\":\"nn.Tanh\",\n",
    "                                \"activation_params\":{\n",
    "                                },\n",
    "                            },\n",
    "                            \"padding_mode\":'zeros',\n",
    "                           },\n",
    "                      \"4\": {\n",
    "                            \"out_channels\":1,\n",
    "                            \"kernel_size\":1, \n",
    "                            \"stride\":1,\n",
    "                            \"padding\":0,\n",
    "                            \"activation\":{\n",
    "                                \"activation_fn\":\"nn.Tanh\",\n",
    "                                \"activation_params\":{\n",
    "                                },\n",
    "                            },\n",
    "                            \"padding_mode\":'zeros',\n",
    "                           },\n",
    "                  },\n",
    "                  \"last_conv2d\":{\n",
    "                      \"kernel_size\":1, \n",
    "                      \"stride\":1,\n",
    "                      \"padding\":0,\n",
    "                    }\n",
    "                }\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'act' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mact\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'act' is not defined"
     ]
    }
   ],
   "source": [
    "act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "forward_params = { \n",
    "        'WaveformNet':{\n",
    "            'in_channels': 1, #WaveformNet \n",
    "            'encoder_channels': [32, 64, 128, 256, 512], #WaveformNet \n",
    "            'decoder_channels': [256, 128, 64, 5],#WaveformNet \n",
    "            },\n",
    "         'FNO':{\n",
    "             'modes1': 30, #FNO \n",
    "             'modes2': 30, #FNO \n",
    "             'width': 32, #FNO \n",
    "             'out_dim':5 #FNO\n",
    "          },  \n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "forward_params.setdefault('fancymodel', {})\n",
    "forward_params['fancymodel']['cfg_path'] = \"e;;p\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'WaveformNet': {'in_channels': 1,\n",
       "  'encoder_channels': [32, 64, 128, 256, 512],\n",
       "  'decoder_channels': [256, 128, 64, 5],\n",
       "  'cfg_path': 'e;;p'},\n",
       " 'FNO': {'modes1': 30, 'modes2': 30, 'width': 32, 'out_dim': 5},\n",
       " 'fancymodel': {'cfg_path': 'e;;p'}}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forward_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "forward_params['iUnet'] = \"test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 8, 334, 70]             288\n",
      "       BatchNorm2d-2           [-1, 8, 334, 70]              16\n",
      "         LeakyReLU-3           [-1, 8, 334, 70]               0\n",
      "            Conv2d-4          [-1, 16, 165, 70]             912\n",
      "       BatchNorm2d-5          [-1, 16, 165, 70]              32\n",
      "         LeakyReLU-6          [-1, 16, 165, 70]               0\n",
      "            Conv2d-7           [-1, 32, 82, 70]           2,592\n",
      "       BatchNorm2d-8           [-1, 32, 82, 70]              64\n",
      "         LeakyReLU-9           [-1, 32, 82, 70]               0\n",
      "           Conv2d-10           [-1, 64, 78, 70]          10,304\n",
      "      BatchNorm2d-11           [-1, 64, 78, 70]             128\n",
      "        LeakyReLU-12           [-1, 64, 78, 70]               0\n",
      "           Conv2d-13          [-1, 128, 74, 70]          41,088\n",
      "      BatchNorm2d-14          [-1, 128, 74, 70]             256\n",
      "        LeakyReLU-15          [-1, 128, 74, 70]               0\n",
      "         Upsample-16          [-1, 128, 70, 70]               0\n",
      "  ConvTranspose2d-17         [-1, 128, 214, 70]         114,816\n",
      "      BatchNorm2d-18         [-1, 128, 214, 70]             256\n",
      "             Tanh-19         [-1, 128, 214, 70]               0\n",
      "  ConvTranspose2d-20          [-1, 64, 433, 70]          57,408\n",
      "      BatchNorm2d-21          [-1, 64, 433, 70]             128\n",
      "             Tanh-22          [-1, 64, 433, 70]               0\n",
      "  ConvTranspose2d-23          [-1, 32, 869, 70]          10,272\n",
      "      BatchNorm2d-24          [-1, 32, 869, 70]              64\n",
      "             Tanh-25          [-1, 32, 869, 70]               0\n",
      "  ConvTranspose2d-26          [-1, 16, 873, 70]           2,576\n",
      "      BatchNorm2d-27          [-1, 16, 873, 70]              32\n",
      "             Tanh-28          [-1, 16, 873, 70]               0\n",
      "  ConvTranspose2d-29           [-1, 5, 877, 70]             405\n",
      "      BatchNorm2d-30           [-1, 5, 877, 70]              10\n",
      "             Tanh-31           [-1, 5, 877, 70]               0\n",
      "           Conv2d-32           [-1, 5, 877, 70]             530\n",
      "         Upsample-33          [-1, 5, 1000, 70]               0\n",
      "================================================================\n",
      "Total params: 242,177\n",
      "Trainable params: 242,177\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 1.34\n",
      "Forward/backward pass size (MB): 207.93\n",
      "Params size (MB): 0.92\n",
      "Estimated Total Size (MB): 210.19\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(amp_ae.to(device), (5, 1000, 70))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.nn.modules.activation.Tanh"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval(\"nn.Tanh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'in_dim': (1000, 70),\n",
       " 'latent_dim': (70, 70),\n",
       " 'in_channels': 5,\n",
       " 'encoder_blocks': {'0': {'out_channels': 8,\n",
       "   'kernel_size': (7, 1),\n",
       "   'stride': (3, 1),\n",
       "   'padding': (3, 0),\n",
       "   'activation': {'activation_fn': 'nn.LeakyReLU',\n",
       "    'activation_params': {'negative_slope': 0.2}},\n",
       "   'padding_mode': 'zeros'},\n",
       "  '1': {'out_channels': 16,\n",
       "   'kernel_size': (7, 1),\n",
       "   'stride': (2, 1),\n",
       "   'padding': (1, 0),\n",
       "   'activation': {'activation_fn': 'nn.LeakyReLU',\n",
       "    'activation_params': {'negative_slope': 0.2}},\n",
       "   'padding_mode': 'zeros'},\n",
       "  '2': {'out_channels': 32,\n",
       "   'kernel_size': (5, 1),\n",
       "   'stride': (2, 1),\n",
       "   'padding': (1, 0),\n",
       "   'activation': {'activation_fn': 'nn.LeakyReLU',\n",
       "    'activation_params': {'negative_slope': 0.2}},\n",
       "   'padding_mode': 'zeros'},\n",
       "  '3': {'out_channels': 64,\n",
       "   'kernel_size': (5, 1),\n",
       "   'stride': (1, 1),\n",
       "   'padding': (0, 0),\n",
       "   'activation': {'activation_fn': 'nn.LeakyReLU',\n",
       "    'activation_params': {'negative_slope': 0.2}},\n",
       "   'padding_mode': 'zeros'},\n",
       "  '4': {'out_channels': 128,\n",
       "   'kernel_size': (5, 1),\n",
       "   'stride': (1, 1),\n",
       "   'padding': (0, 0),\n",
       "   'activation': {'activation_fn': 'nn.LeakyReLU',\n",
       "    'activation_params': {'negative_slope': 0.2}},\n",
       "   'padding_mode': 'zeros'}},\n",
       " 'decoder_blocks': {'0': {'out_channels': 128,\n",
       "   'kernel_size': (7, 1),\n",
       "   'stride': (3, 1),\n",
       "   'padding': (3, 0),\n",
       "   'activation': {'activation_fn': 'nn.Tanh', 'activation_params': {}},\n",
       "   'padding_mode': 'zeros'},\n",
       "  '1': {'out_channels': 64,\n",
       "   'kernel_size': (7, 1),\n",
       "   'stride': (2, 1),\n",
       "   'padding': (1, 0),\n",
       "   'activation': {'activation_fn': 'nn.Tanh', 'activation_params': {}},\n",
       "   'padding_mode': 'zeros'},\n",
       "  '2': {'out_channels': 32,\n",
       "   'kernel_size': (5, 1),\n",
       "   'stride': (2, 1),\n",
       "   'padding': (1, 0),\n",
       "   'activation': {'activation_fn': 'nn.Tanh', 'activation_params': {}},\n",
       "   'padding_mode': 'zeros'},\n",
       "  '3': {'out_channels': 16,\n",
       "   'kernel_size': (5, 1),\n",
       "   'stride': (1, 1),\n",
       "   'padding': (0, 0),\n",
       "   'activation': {'activation_fn': 'nn.Tanh', 'activation_params': {}},\n",
       "   'padding_mode': 'zeros'},\n",
       "  '4': {'out_channels': 5,\n",
       "   'kernel_size': (5, 1),\n",
       "   'stride': (1, 1),\n",
       "   'padding': (0, 0),\n",
       "   'activation': {'activation_fn': 'nn.Tanh', 'activation_params': {}},\n",
       "   'padding_mode': 'zeros'},\n",
       "  'last_conv2d': {'kernel_size': (7, 3), 'stride': (3, 1), 'padding': (0, 0)}}}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amp_params_70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('../configs/velocity_config_latent_dim_70.json', 'w') as file:\n",
    "    json.dump(vel_params_70, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ConvBlockLegacy(nn.Module):\n",
    "    \"\"\"(convolution => [BatchNorm] => LeakyReLU) * 2\"\"\"\n",
    "    def __init__(self, \n",
    "                 in_channels, \n",
    "                 out_channels,\n",
    "                 activation_fn=nn.LeakyReLU,\n",
    "                 activation_params={},\n",
    "                 kernel_size=(1, 1), \n",
    "                 stride=(1, 1), \n",
    "                 padding=(0, 0),\n",
    "                 padding_mode='zeros',\n",
    "                 transpose_conv=False,\n",
    "                ):\n",
    "        super().__init__()\n",
    "        conv_fn = nn.ConvTranspose2d if transpose_conv else nn.Conv2d\n",
    "        self.conv_block = nn.Sequential(\n",
    "            conv_fn(in_channels, out_channels, kernel_size=kernel_size, \n",
    "                    stride=stride, padding=padding, padding_mode=padding_mode),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            activation_fn(**activation_params),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.conv_block(x)\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    \"\"\"(convolution => [GroupNorm] => LeakyReLU) * 2\"\"\"\n",
    "    def __init__(self, \n",
    "                 in_channels, \n",
    "                 out_channels, \n",
    "                 kernel_size=(1, 1), \n",
    "                 stride=(1, 1), \n",
    "                 padding=(0, 0),\n",
    "                 negative_slope=0.01,\n",
    "                 groups=1,\n",
    "                 transpose_conv=False,\n",
    "                ):\n",
    "        super().__init__()\n",
    "        conv_fn = nn.ConvTranspose2d if transpose_conv else nn.Conv2d\n",
    "        self.conv_block = nn.Sequential(\n",
    "            conv_fn(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding),\n",
    "            nn.GroupNorm(groups, out_channels, 1e-3),\n",
    "            nn.LeakyReLU(negative_slope=negative_slope, inplace=True),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.conv_block(x)\n",
    "\n",
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, cfg_path=\"./configs/\", cfg_name='amplitude_config_latent_dim_70.json'):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        \n",
    "        config_file = os.path.join(cfg_path, cfg_name)\n",
    "        with open(config_file, 'r') as file:\n",
    "            cfg = json.load(file)\n",
    "        \n",
    "        encoder_layers = nn.ModuleList()\n",
    "        decoder_layers = nn.ModuleList()\n",
    "        \n",
    "        for i, key in enumerate(cfg[\"encoder_blocks\"].keys()):\n",
    "            enc_dict = cfg[\"encoder_blocks\"][key]\n",
    "            activation_fn = eval(enc_dict[\"activation\"][\"activation_fn\"])\n",
    "            activation_params = enc_dict[\"activation\"][\"activation_params\"]\n",
    "            in_channels = cfg[\"in_channels\"] if i==0 else cfg[\"encoder_blocks\"][str(i-1)][\"out_channels\"]\n",
    "            out_channels = enc_dict[\"out_channels\"]\n",
    "            conv_block = ConvBlockLegacy(\n",
    "                             in_channels=in_channels, \n",
    "                             out_channels=out_channels,\n",
    "                             activation_fn=activation_fn,\n",
    "                             activation_params=activation_params,\n",
    "                             kernel_size=enc_dict[\"kernel_size\"], \n",
    "                             stride=enc_dict[\"stride\"], \n",
    "                             padding=enc_dict[\"padding\"],\n",
    "                             padding_mode=enc_dict[\"padding_mode\"],\n",
    "                             transpose_conv=False,          \n",
    "                    )\n",
    "            encoder_layers.append(conv_block)\n",
    "        encoder_layers.append(nn.Upsample(cfg[\"latent_dim\"], mode='bilinear'))\n",
    "        \n",
    "        for i, key in enumerate(cfg[\"decoder_blocks\"].keys()):\n",
    "            dec_dict = cfg[\"decoder_blocks\"][key]\n",
    "            activation_fn = eval(dec_dict[\"activation\"][\"activation_fn\"])\n",
    "            activation_params = dec_dict[\"activation\"][\"activation_params\"]\n",
    "            in_channels = out_channels if i==0 else cfg[\"decoder_blocks\"][str(i-1)][\"out_channels\"]\n",
    "            out_channels = dec_dict[\"out_channels\"]\n",
    "            conv_block = ConvBlockLegacy(\n",
    "                             in_channels=in_channels, \n",
    "                             out_channels=out_channels,\n",
    "                             activation_fn=activation_fn,\n",
    "                             activation_params=activation_params,\n",
    "                             kernel_size=dec_dict[\"kernel_size\"], \n",
    "                             stride=dec_dict[\"stride\"], \n",
    "                             padding=dec_dict[\"padding\"],\n",
    "                             padding_mode=dec_dict[\"padding_mode\"],\n",
    "                             transpose_conv=True,          \n",
    "                    )\n",
    "            decoder_layers.append(conv_block)\n",
    "        \n",
    "        #last convolutional block after decoder\n",
    "        last_conv = nn.Conv2d(\n",
    "                             in_channels=out_channels, \n",
    "                             out_channels=out_channels,\n",
    "                             kernel_size=cfg[\"last_conv2d\"][\"kernel_size\"], \n",
    "                             stride=cfg[\"last_conv2d\"][\"stride\"], \n",
    "                             padding=cfg[\"last_conv2d\"][\"padding\"],        \n",
    "                    )\n",
    "        decoder_layers.append(last_conv)\n",
    "        decoder_layers.append(nn.Upsample(cfg[\"in_dim\"], mode='bilinear'))\n",
    "        \n",
    "        self.encoder_layers = nn.Sequential(*encoder_layers)\n",
    "        self.decoder_layers = nn.Sequential(*decoder_layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.encoder_layers(x)\n",
    "        x = self.decoder_layers(x)\n",
    "        return x\n",
    "    \n",
    "    def embedding(self, x):\n",
    "        x = self.encoder_layers(x)\n",
    "        return x\n",
    "    \n",
    "    def decoder(self, x):\n",
    "        x = self.decoder_layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder = AutoEncoder(cfg_path=\"/home/darka/projects/Latent_Bijectivity/configs/\", \n",
    "                          cfg_name='amplitude_config_latent_dim_70.json'\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 8, 334, 70]             288\n",
      "       BatchNorm2d-2           [-1, 8, 334, 70]              16\n",
      "         LeakyReLU-3           [-1, 8, 334, 70]               0\n",
      "   ConvBlockLegacy-4           [-1, 8, 334, 70]               0\n",
      "            Conv2d-5          [-1, 16, 165, 70]             912\n",
      "       BatchNorm2d-6          [-1, 16, 165, 70]              32\n",
      "         LeakyReLU-7          [-1, 16, 165, 70]               0\n",
      "   ConvBlockLegacy-8          [-1, 16, 165, 70]               0\n",
      "            Conv2d-9           [-1, 32, 82, 70]           2,592\n",
      "      BatchNorm2d-10           [-1, 32, 82, 70]              64\n",
      "        LeakyReLU-11           [-1, 32, 82, 70]               0\n",
      "  ConvBlockLegacy-12           [-1, 32, 82, 70]               0\n",
      "           Conv2d-13           [-1, 64, 78, 70]          10,304\n",
      "      BatchNorm2d-14           [-1, 64, 78, 70]             128\n",
      "        LeakyReLU-15           [-1, 64, 78, 70]               0\n",
      "  ConvBlockLegacy-16           [-1, 64, 78, 70]               0\n",
      "           Conv2d-17          [-1, 128, 74, 70]          41,088\n",
      "      BatchNorm2d-18          [-1, 128, 74, 70]             256\n",
      "        LeakyReLU-19          [-1, 128, 74, 70]               0\n",
      "  ConvBlockLegacy-20          [-1, 128, 74, 70]               0\n",
      "         Upsample-21          [-1, 128, 70, 70]               0\n",
      "  ConvTranspose2d-22         [-1, 128, 208, 70]         114,816\n",
      "      BatchNorm2d-23         [-1, 128, 208, 70]             256\n",
      "             Tanh-24         [-1, 128, 208, 70]               0\n",
      "  ConvBlockLegacy-25         [-1, 128, 208, 70]               0\n",
      "  ConvTranspose2d-26          [-1, 64, 419, 70]          57,408\n",
      "      BatchNorm2d-27          [-1, 64, 419, 70]             128\n",
      "             Tanh-28          [-1, 64, 419, 70]               0\n",
      "  ConvBlockLegacy-29          [-1, 64, 419, 70]               0\n",
      "  ConvTranspose2d-30          [-1, 32, 839, 70]          10,272\n",
      "      BatchNorm2d-31          [-1, 32, 839, 70]              64\n",
      "             Tanh-32          [-1, 32, 839, 70]               0\n",
      "  ConvBlockLegacy-33          [-1, 32, 839, 70]               0\n",
      "  ConvTranspose2d-34          [-1, 16, 843, 70]           2,576\n",
      "      BatchNorm2d-35          [-1, 16, 843, 70]              32\n",
      "             Tanh-36          [-1, 16, 843, 70]               0\n",
      "  ConvBlockLegacy-37          [-1, 16, 843, 70]               0\n",
      "  ConvTranspose2d-38           [-1, 5, 847, 70]             405\n",
      "      BatchNorm2d-39           [-1, 5, 847, 70]              10\n",
      "             Tanh-40           [-1, 5, 847, 70]               0\n",
      "  ConvBlockLegacy-41           [-1, 5, 847, 70]               0\n",
      "           Conv2d-42           [-1, 5, 847, 70]             530\n",
      "         Upsample-43          [-1, 5, 1000, 70]               0\n",
      "================================================================\n",
      "Total params: 242,177\n",
      "Trainable params: 242,177\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 1.34\n",
      "Forward/backward pass size (MB): 266.94\n",
      "Params size (MB): 0.92\n",
      "Estimated Total Size (MB): 269.20\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(autoencoder.to(device), (5, 1000, 70))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
