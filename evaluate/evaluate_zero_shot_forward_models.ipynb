{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57436df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "import openpyxl\n",
    "from openpyxl import Workbook\n",
    "\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torchvision.transforms import Compose\n",
    "from torch.utils.data import RandomSampler, DataLoader\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.nn.parallel import DistributedDataParallel\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "#from tqdm.notebook import tqdm\n",
    "# from ipywidgets import FloatProgress\n",
    "\n",
    "from iunets import iUNet\n",
    "from dataset import FWIDataset\n",
    "# from networks import iunet_network\n",
    "from networks import forward_network, inverse_network, iunet_network, autoencoder\n",
    "\n",
    "import utils.transforms as T\n",
    "from utils.pytorch_ssim import *\n",
    "import utils.utilities as utils\n",
    "from utils.scheduler import WarmupMultiStepLR\n",
    "from utils.config_utils import get_config_name, get_latent_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a67af973",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import torch\n",
    "\n",
    "parser = argparse.ArgumentParser(description=\"OpenFWI arguments\")\n",
    "\n",
    "# Define command-line arguments\n",
    "parser.add_argument(\"--step\", type=int, default=0, help=\"Step value\")\n",
    "parser.add_argument(\"--file_size\", type=int, default=500, help=\"File size\")\n",
    "parser.add_argument(\"--vis_suffix\", action=\"store_true\", help=\"Enable visualization suffix\")\n",
    "parser.add_argument(\"--device\", default=\"cuda\", choices=[\"cpu\", \"cuda\"], help=\"Device (cpu or cuda)\")\n",
    "\n",
    "parser.add_argument(\"--k\", type=int, default=1, help=\"Value for k\")\n",
    "parser.add_argument(\"--workers\", type=int, default=4, help=\"Number of workers\")\n",
    "parser.add_argument(\"--lambda_g1v\", type=int, default=1, help=\"Value for lambda_g1v\")\n",
    "parser.add_argument(\"--lambda_g2v\", type=int, default=1, help=\"Value for lambda_g2v\")\n",
    "parser.add_argument(\"--batch_size\", type=int, default=50, help=\"Batch size\")\n",
    "parser.add_argument(\"--mask_factor\", type=float, default=0.0, help=\"Mask factor\")\n",
    "parser.add_argument(\"--sample_temporal\", type=int, default=1, help=\"Temporal sampling value\")\n",
    "parser.add_argument(\"--distributed\", action=\"store_true\", help=\"Enable distributed computing\")\n",
    "\n",
    "parser.add_argument(\"--num_images\", type=int, default=5, help=\"Number of images\")\n",
    "\n",
    "parser.add_argument(\"--model_type\", default=\"IUnetForwardModel\", help=\"Model type\")\n",
    "parser.add_argument(\"--base_path\", default=\"/projects/ml4science/OpenFWI/Results/SupervisedExperiment/\", help=\"Base path\")\n",
    "parser.add_argument(\"--model_save_name\", default=\"IUnetForwardModel\", help=\"Model save name\")\n",
    "parser.add_argument(\"--unet_depth\", type=int, default=2, help=\"UNet depth\")\n",
    "parser.add_argument(\"--unet_repeat_blocks\", type=int, default=2, help=\"Number of repeated UNet blocks\")\n",
    "parser.add_argument(\"--latent_dim\", type=int, default=70, help=\"Latent Dimension\")\n",
    "parser.add_argument(\"--skip\", type=int, default=1, help=\"Skip Connections for UNet\")\n",
    "parser.add_argument(\"--cfg_path\", default=\"../configs/\", help=\"Cfg path\")\n",
    "\n",
    "\n",
    "\n",
    "# Parse the command-line arguments\n",
    "args = parser.parse_args(args=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e62deebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reinitialize variables without the 'args' prefix\n",
    "for key, value in vars(args).items():\n",
    "    exec(f\"{key} = value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8cf6769",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_transform(data):\n",
    "    return torch.log10(1+torch.abs(data)) * torch.sign(data)\n",
    "\n",
    "def tanh_transform(data):\n",
    "    return torch.nn.functional.tanh(data)\n",
    "\n",
    "datasets = [\"flatvel-a\", \"flatvel-b\",\n",
    "          \"curvevel-a\", \"curvevel-b\",\n",
    "          \"flatfault-a\", \"flatfault-b\",\n",
    "          \"curvefault-a\", \"curvefault-b\",\n",
    "          \"style-a\", \"style-b\"]\n",
    "\n",
    "model_names = ['FlatVel-A', 'FlatVel-B',\n",
    "          'CurveVel-A', 'CurveVel-B',\n",
    "         'FlatFault-A', 'FlatFault-B',\n",
    "         'CurveFault-A', 'CurveFault-B',\n",
    "         'Style-A', 'Style-B']\n",
    "\n",
    "\n",
    "datasets = [\"flatvel-a\", \"flatvel-b\"]\n",
    "\n",
    "model_names = ['FlatVel-A', 'FlatVel-B']\n",
    "\n",
    "\n",
    "\n",
    "# datasets = [\"flatvel-a\"]\n",
    "# model_names = ['FlatVel-A']\n",
    "model_type = \"FNO\"\n",
    "model_save_name = \"FNO\"\n",
    "\n",
    "# #list of datasets on which model is evaluated\n",
    "# datasets = [\"flatvel-a\", \"flatvel-b\"]\n",
    "# #list of the datasets the model has been trained on\n",
    "# model_names = [\"FlatVel-A\", \"FlatVel-B\"] \n",
    "\n",
    "model_paths = []\n",
    "for model_name in model_names:\n",
    "    path_ = os.path.join(model_name, model_save_name, \"fcn_l1loss_ffb\")\n",
    "    model_paths.append(path_)\n",
    "\n",
    "architecture_params = {\"UNetForwardModel_17M\":{\"unet_depth\": 2, \"unet_repeat_blocks\": 1}, \n",
    "                       \"UNetForwardModel_33M\":{\"unet_depth\": 2, \"unet_repeat_blocks\": 2},\n",
    "                       \"default\":{\"unet_depth\": 2, \"unet_repeat_blocks\": 2}\n",
    "                      }\n",
    "                  \"CurveVel-A/Joint/Cycle_Loss_0_Mask_Factor_80/lambda_amp_10_lambda_vel_1/fcn_l1loss_ffb\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c367eec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterions = {\n",
    "    'MAE': lambda x, y: torch.mean(torch.abs(x - y)),\n",
    "    'MSE': lambda x, y: torch.mean((x - y) ** 2)\n",
    "}\n",
    "\n",
    "def get_dataset_path(dataset):\n",
    "    arr = dataset.split(\"-\")\n",
    "    base_path = f\"../train_test_splits/\"\n",
    "    \n",
    "    train_path = os.path.join(base_path, f\"{arr[0]}_{arr[1]}_train.txt\")\n",
    "    val_path = os.path.join(base_path, f\"{arr[0]}_{arr[1]}_val.txt\")\n",
    "    \n",
    "    return train_path, val_path\n",
    "\n",
    "def get_transforms(dataset, return_ctx=False):\n",
    "    f = open('../dataset_config.json')\n",
    "    ctx = json.load(f)[dataset]\n",
    "\n",
    "    transform_data = T.Normalize(ctx['data_mean'], ctx['data_std'])\n",
    "    transform_label = T.MinMaxNormalize(ctx['label_min'], ctx['label_max'])\n",
    "    if return_ctx:\n",
    "        return  transform_data, transform_label, ctx\n",
    "    return  transform_data, transform_label\n",
    "\n",
    "\n",
    "def get_dataloader(test_dataset, train_dataset=None):\n",
    "    if train_dataset is None:\n",
    "        train_dataset = test_dataset\n",
    "    \n",
    "    transform_data, transform_label, ctx = get_transforms(train_dataset, return_ctx=True)\n",
    "\n",
    "    train_anno, val_anno = get_dataset_path(test_dataset)\n",
    "        \n",
    "    print(f'Loading {test_dataset} validation data')\n",
    "    dataset_valid = FWIDataset(\n",
    "        val_anno,\n",
    "        preload=True,\n",
    "        sample_ratio=sample_temporal,\n",
    "        file_size=ctx['file_size'],\n",
    "        transform_data=transform_data,\n",
    "        transform_label=transform_label\n",
    "    )\n",
    "        \n",
    "    valid_sampler = RandomSampler(dataset_valid)\n",
    "\n",
    "    dataloader_valid = DataLoader(\n",
    "                                dataset_valid, batch_size=batch_size,\n",
    "                                sampler=valid_sampler, num_workers=workers,\n",
    "                                pin_memory=True, collate_fn=default_collate)\n",
    "    \n",
    "    print('Data loading over')\n",
    "        \n",
    "    return dataset_valid, dataloader_valid, transform_data, transform_label \n",
    "\n",
    "\n",
    "def evaluate(model, dataloader, transform_data, transform_label, k, criterions, device):   \n",
    "    eval_metrics = {}\n",
    "    \n",
    "    amp_list, amp_pred_list= [], [] # store denormalized velocity predcition & gt in numpy \n",
    "    amp_norm_list,amp_pred_norm_list = [], [] # store normalized velocity prediction & gt in tensor\n",
    "\n",
    "    ssim_loss = SSIM(window_size=11)\n",
    "    ssim_value = 0\n",
    "    with torch.no_grad():\n",
    "        batch_idx = 0\n",
    "        for _, amp, vel in dataloader:\n",
    "            amp = amp.to(device)\n",
    "            vel = vel.to(device)\n",
    "#             print(\"Getting prediction from model\")\n",
    "            amp_pred = model(vel)\n",
    "#             print(\"model prediction received\")\n",
    "\n",
    "            ssim_value += ssim_loss(amp / 2 + 0.5, amp_pred / 2 + 0.5).item()\n",
    "    \n",
    "            amp_np = transform_data.inverse_transform(amp.detach().cpu().numpy())\n",
    "            amp_list.append(torch.from_numpy(amp_np))\n",
    "            amp_norm_list.append(amp.detach().cpu())\n",
    "            \n",
    "            amp_pred_np = transform_data.inverse_transform(amp_pred.detach().cpu().numpy())\n",
    "            amp_pred_list.append(torch.from_numpy(amp_pred_np))\n",
    "            amp_pred_norm_list.append(amp_pred.detach().cpu())\n",
    "            \n",
    "            batch_idx += 1\n",
    "\n",
    "    amp, amp_pred = torch.cat(amp_list), torch.cat(amp_pred_list)\n",
    "    amp_norm, amp_pred_norm = torch.cat(amp_norm_list), torch.cat(amp_pred_norm_list)\n",
    "    \n",
    "    amp_log, amp_pred_log = log_transform(amp), log_transform(amp_pred)\n",
    "    amp_tanh, amp_pred_tanh = tanh_transform(amp), tanh_transform(amp_pred)\n",
    "    \n",
    "    for name, criterion in criterions.items():\n",
    "        \n",
    "        eval_metrics[f'Waveform_norm_{name}'] = criterion(amp_norm, amp_pred_norm).item()\n",
    "        eval_metrics[f'Waveform_unnorm_{name}'] = criterion(amp, amp_pred).item()\n",
    "        \n",
    "        eval_metrics[f'Waveform_unnorm_log_{name}'] = criterion(amp_log, amp_pred_log).item()\n",
    "        eval_metrics[f'Waveform_unnorm_tanh_{name}'] = criterion(amp_tanh, amp_pred_tanh).item()\n",
    "        \n",
    "    eval_metrics[f'Waveform_SSIM']  = ssim_value/len(dataloader) \n",
    "#     eval_metrics[f'Waveform_SSIM'] = ssim_loss(amp_norm / 2 + 0.5, amp_pred_norm / 2 + 0.5).item()\n",
    "    return eval_metrics\n",
    "\n",
    "def set_forward_params(forward_model_params):\n",
    "        forward_model_params.setdefault('IUnetForwardModel', {})\n",
    "        forward_model_params['IUnetForwardModel']['cfg_path'] = cfg_path\n",
    "        forward_model_params['IUnetForwardModel']['latent_dim'] = latent_dim\n",
    "        \n",
    "        forward_model_params.setdefault('UNetForwardModel', {})\n",
    "        forward_model_params['UNetForwardModel']['cfg_path'] = cfg_path\n",
    "        forward_model_params['UNetForwardModel']['latent_dim'] = latent_dim\n",
    "        forward_model_params['UNetForwardModel']['unet_depth'] = architecture_params[\"default\"][\"unet_depth\"]\n",
    "        forward_model_params['UNetForwardModel']['unet_repeat_blocks'] = architecture_params[\"default\"][\"unet_repeat_blocks\"]\n",
    "        forward_model_params['UNetForwardModel']['skip'] = skip # skip true\n",
    "        return forward_model_params\n",
    "\n",
    "def get_model(model_path, model_type):\n",
    "    try:\n",
    "        print(model_path)\n",
    "        forward_model_params = forward_network.forward_params\n",
    "        forward_model_params = set_forward_params(forward_model_params)\n",
    "        model = forward_network.model_dict[model_type](**forward_model_params[model_type]).to(device)\n",
    "        checkpoint = torch.load(model_path, map_location=torch.device('cpu'))\n",
    "        model.load_state_dict(checkpoint['model'])\n",
    "    except:\n",
    "        print(\"Failed to load new model. Falling back to Legacy Code.\")\n",
    "        forward_model_params = forward_network.forward_params_legacy\n",
    "        forward_model_params['unet_depth'] = unet_depth\n",
    "        forward_model_params['unet_repeat_blocks'] = unet_repeat_blocks\n",
    "        model_type = model_type+\"_Legacy\"\n",
    "        model = forward_network.model_dict[model_type](**forward_model_params).to(device)\n",
    "        checkpoint = torch.load(model_path, map_location=torch.device('cpu'))\n",
    "        model.load_state_dict(checkpoint['model'])\n",
    "\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    return model\n",
    "\n",
    "def generate_eval_matrix(model_names, model_paths, datasets):\n",
    "    \n",
    "    metrics = {}\n",
    "\n",
    "    for i in range(len(model_names)):\n",
    "        model_name = model_names[i]\n",
    "        print(f'------------ Outer Loop: Evaluating {model_name} ------------')\n",
    "        model_path = os.path.join(base_path, model_paths[i], \"latest_checkpoint.pth\")\n",
    "        if not os.path.exists(model_path):\n",
    "            print(f\"The path does not exist: {model_path}\")\n",
    "            continue\n",
    "        model = get_model(model_path, model_type)\n",
    "        \n",
    "        vis_path = os.path.join(base_path, model_paths[i], 'Zero_Shot_Generalization')\n",
    "        if not os.path.exists(vis_path):\n",
    "            os.makedirs(vis_path)\n",
    "        \n",
    "        # We need the transform_data/transform_label for the trained model.\n",
    "        model_train_dataset = datasets[i]\n",
    "        \n",
    "        model_metrics = {}\n",
    "        for dataset in datasets:\n",
    "            dataset_val, dataloader_val,transform_data, transform_label  = get_dataloader(test_dataset=dataset, train_dataset=model_train_dataset)\n",
    "            print(f'------------ Inner Loop Evaluating {dataset} ------------')\n",
    "            eval_dict = evaluate(model, dataloader_val, transform_data, transform_label, k, criterions, device)\n",
    "            utils.plot_images(num_images, dataset_val, model, dataset, vis_path, device, transform_data, transform_label, plot=False, save_key=\"dataset\")\n",
    "            \n",
    "            for key in eval_dict.keys():\n",
    "                model_metrics.setdefault(key, []).append(eval_dict[key])\n",
    "                \n",
    "        metrics[model_name] = model_metrics\n",
    "    return metrics\n",
    "\n",
    "def write_metrics(metrics_dict, filename):\n",
    "    \n",
    "    workbook = Workbook()\n",
    "    workbook.remove(workbook.active)\n",
    "    metrics_list = list(metrics_dict.values())[0].keys()\n",
    "    \n",
    "    for metric in metrics_list:\n",
    "        workbook.create_sheet(title=metric)\n",
    "        sheet = workbook[metric]\n",
    "        \n",
    "        for idx, dataset in enumerate(datasets, start=1):\n",
    "            cell = sheet.cell(row=1, column=idx+1, value=dataset)\n",
    "            cell.font = openpyxl.styles.Font(bold=True)\n",
    "            cell.alignment = openpyxl.styles.Alignment(horizontal='center')\n",
    "            \n",
    "        \n",
    "        for idx, model_name in enumerate(metrics_dict.keys(), start=1):\n",
    "            cell = sheet.cell(row=idx+1, column=1, value=model_name)\n",
    "            cell.alignment = openpyxl.styles.Alignment(horizontal='center')\n",
    "            \n",
    "            metric_values = metrics_dict[model_name][metric]\n",
    "            \n",
    "            for col, val in enumerate(metric_values, start=1):\n",
    "                cell = sheet.cell(row=idx+1, column=col+1, value=val)\n",
    "                cell.alignment = openpyxl.styles.Alignment(horizontal='center')\n",
    "    \n",
    "    workbook.save(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a27daf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------ Outer Loop: Evaluating FlatVel-A ------------\n",
      "/projects/ml4science/OpenFWI/Results/SupervisedExperiment/FlatVel-A/FNO/fcn_l1loss_ffb/latest_checkpoint.pth\n",
      "Loading flatvel-a validation data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:17<00:00,  1.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data concatenation complete.\n",
      "Data loading over\n",
      "------------ Inner Loop Evaluating flatvel-a ------------\n",
      "Loading flatvel-b validation data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:13<00:00,  1.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data concatenation complete.\n",
      "Data loading over\n",
      "------------ Inner Loop Evaluating flatvel-b ------------\n",
      "------------ Outer Loop: Evaluating FlatVel-B ------------\n",
      "/projects/ml4science/OpenFWI/Results/SupervisedExperiment/FlatVel-B/FNO/fcn_l1loss_ffb/latest_checkpoint.pth\n",
      "Loading flatvel-a validation data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:15<00:00,  1.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data concatenation complete.\n",
      "Data loading over\n",
      "------------ Inner Loop Evaluating flatvel-a ------------\n",
      "Loading flatvel-b validation data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:13<00:00,  1.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data concatenation complete.\n",
      "Data loading over\n",
      "------------ Inner Loop Evaluating flatvel-b ------------\n"
     ]
    }
   ],
   "source": [
    "eval_matrix = generate_eval_matrix(model_names, model_paths, datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b36209d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_metrics(eval_matrix, f'eval_metric{model_save_name}.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6c314a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openfwi_env",
   "language": "python",
   "name": "openfwi_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
