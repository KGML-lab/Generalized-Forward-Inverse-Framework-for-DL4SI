{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "print(sys.path)\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "import openpyxl\n",
    "from openpyxl import Workbook\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import TwoSlopeNorm\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torchvision.transforms import Compose\n",
    "from torch.utils.data import RandomSampler, DataLoader\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.nn.parallel import DistributedDataParallel\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from iunets import iUNet\n",
    "from dataset import FWIDataset\n",
    "# from networks import iunet_network\n",
    "from networks import forward_network, inverse_network, iunet_network\n",
    "\n",
    "import utils.transforms as T\n",
    "from utils.pytorch_ssim import *\n",
    "import utils.utilities as utils\n",
    "from utils.scheduler import WarmupMultiStepLR\n",
    "from matplotlib.colors import ListedColormap\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "rainbow_cmap = ListedColormap(np.load('/projects/ml4science/OpenFWI/Latent_Bijectivity/utils/rainbow256.npy'))\n",
    "\n",
    "forward_model_list = [forward_network.FNO2d, forward_network.WaveformNet, forward_network.WaveformNet_V2, iunet_network.IUnetForwardModel, iunet_network.UNetForwardModel]\n",
    "inverse_model_list = [inverse_network.InversionNet, iunet_network.IUnetInverseModel, iunet_network.UNetInverseModel]\n",
    "joint_model_list = [iunet_network.IUnetModel, iunet_network.JointModel, iunet_network.Decouple_IUnetModel]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 0\n",
    "file_size = 500\n",
    "vis_suffix = False\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "k = 1\n",
    "workers = 4\n",
    "lambda_g1v = 1\n",
    "lambda_g2v = 1\n",
    "batch_size = 50\n",
    "mask_factor = 0.0\n",
    "sample_temporal = 1\n",
    "distributed = False\n",
    "\n",
    "num_images = 2\n",
    "\n",
    "cfg_path = \"../../configs/\"\n",
    "latent_dim = 70\n",
    "skip = 0\n",
    "\n",
    "\n",
    "base_path = \"/projects/ml4science/OpenFWI/Results/SupervisedExperiment/\"\n",
    "\n",
    "mode = \"appendix\" #\"appendix\" or \"main_paper\"\n",
    "viz_save_path = f'AAAI_viz_aug15/{mode}'\n",
    "\n",
    "font_sizes = {\"color_bar\": 16, \"sub_plt_title\":20, \"dataset_name\": 20}\n",
    "\n",
    "\n",
    "evaluate_datasets = [\"marmousi\", \"marmousi_smooth\"]\n",
    "model_train_datasets = ['FlatVel-A', 'FlatVel-B', 'CurveVel-A', 'CurveVel-B',\n",
    "                        'FlatFault-A', 'FlatFault-B', 'CurveFault-A', 'CurveFault-B', 'Style-A', 'Style-B']\n",
    "\n",
    "if mode == \"appendix\":\n",
    "    architecture_types =  [\"InversionNet\",  \"InversionNet\", \"AutoLinearInverse\", \"UNetInverseModel\", \"UNetInverseModel\",  \n",
    "                      \"IUNET\", \"IUNET\"]\n",
    "                     \n",
    "    architecture_names =  [\"InversionNet\", \"Velocity_GAN\", \"AutoLinear_Inversion_ckpt\", \"UNetInverseModel_17M\", \"UNetInverseModel_33M\", \n",
    "                     \"Invertible_XNet\", \"Invertible_XNet_cycle_warmup\"]\n",
    "else:\n",
    "\n",
    "    architecture_types =  [\"InversionNet\",  \"AutoLinearInverse\", \"UNetInverseModel\", \"IUNET\"]\n",
    "\n",
    "    architecture_names =  [\"InversionNet\", \"AutoLinear_Inversion_ckpt\", \"UNetInverseModel_33M\", \n",
    "                           \"Invertible_XNet\"]\n",
    "\n",
    "\n",
    "plot_names = {\"InversionNet\": \"InversionNet\", \"UNetInverseModel_17M\": \"Latent U-Net (Small)\", \n",
    "              \"UNetInverseModel_33M\": \"Latent U-Net (Large)\",\n",
    "              \"IUnetInverseModel\": \"IUnetInverseModel\" ,\n",
    "              \"Invertible_XNet\": \"Invertible X-Net\", \"Invertible_XNet_cycle_warmup\": \"Invertible X-Net (Cycle)\",\n",
    "             \"Velocity_GAN\": \"VelocityGAN\", \"ground_truth\": \"Ground Truth\",\n",
    "             \"AutoLinear_Inversion_ckpt\": \"Auto-Linear\"}\n",
    "\n",
    "plot_dataset_names = {\"flatvel-a\": \"FVA\", \"flatvel-b\": \"FVB\", \"curvevel-a\": \"CVA\", \"curvevel-b\": \"CVB\",\n",
    "                     \"flatfault-a\": \"FFA\", \"flatfault-b\": \"FFB\",  \"curvefault-a\": \"CFA\", \"curvefault-b\": \"CFB\", \n",
    "                      \"style-a\": \"STA\", \"style-b\": \"STB\", \n",
    "                      \"marmousi_0\": \"Marmousi\", \"marmousi_smooth_0\": \"Marmousi smooth\",\n",
    "                      \"marmousi_1\": \"Overthrust\", \"marmousi_smooth_1\": \"Overthrust smooth\"}\n",
    "                       \n",
    "architecture_params = {\"UNetInverseModel_17M\":{\"unet_depth\": 2, \"unet_repeat_blocks\": 1}, \n",
    "                       \"UNetInverseModel_33M\":{\"unet_depth\": 2, \"unet_repeat_blocks\": 2},\n",
    "                       \"default\":{\"unet_depth\": 2, \"unet_repeat_blocks\": 2}\n",
    "                      }\n",
    "model_paths = {}\n",
    "for model_name in model_train_datasets:\n",
    "    model_paths[model_name] = {}\n",
    "    for i, architecture_name in enumerate(architecture_names):\n",
    "        path_ = os.path.join(model_name, architecture_name, \"fcn_l1loss_ffb\")\n",
    "        model_paths[model_name][architecture_name]= path_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linestyle_tuple = [\n",
    "     ('loosely dotted',        (0, (1, 10))),\n",
    "     ('dotted',                (0, (1, 1))),\n",
    "     ('densely dotted',        (0, (1, 1))),\n",
    "     ('long dash with offset', (5, (10, 3))),\n",
    "     ('loosely dashed',        (0, (5, 10))),\n",
    "     ('dashed',                (0, (5, 5))),\n",
    "     ('densely dashed',        (0, (5, 1))),\n",
    "\n",
    "     ('loosely dashdotted',    (0, (3, 10, 1, 10))),\n",
    "     ('dashdotted',            (0, (3, 5, 1, 5))),\n",
    "     ('densely dashdotted',    (0, (3, 1, 1, 1))),\n",
    "\n",
    "     ('dashdotdotted',         (0, (3, 5, 1, 5, 1, 5))),\n",
    "     ('loosely dashdotdotted', (0, (3, 10, 1, 10, 1, 10))),\n",
    "     ('densely dashdotdotted', (0, (3, 1, 1, 1, 1, 1)))]\n",
    "\n",
    "linestyle_dict = {name: style for name, style in linestyle_tuple}\n",
    "\n",
    "plot_map = {\n",
    "                \"ground_truth\":{\n",
    "                   \"linestyle\":\"solid\", \n",
    "                   \"color\":\"black\",\n",
    "                   \"zorder\":0,\n",
    "                  },\n",
    "                 \"InversionNet\":{\n",
    "                   \"linestyle\":linestyle_dict[\"dashed\"], \n",
    "                   \"color\":\"blue\",\n",
    "                   \"zorder\":3,\n",
    "                  },\n",
    "                 \"UNetInverseModel_33M\":{\n",
    "                   \"linestyle\":linestyle_dict[\"densely dashed\"], \n",
    "                   \"color\":\"red\",\n",
    "                   \"zorder\":2,\n",
    "                  },\n",
    "                  \"Invertible_XNet_cycle_warmup\":{\n",
    "                   \"linestyle\":\"dotted\", \n",
    "                   \"color\":\"darkgreen\",\n",
    "                   \"zorder\":1,\n",
    "                  },\n",
    "           }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterions = {\n",
    "    'MAE': lambda x, y: torch.mean(torch.abs(x - y)),\n",
    "    'MSE': lambda x, y: torch.mean((x - y) ** 2)\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "def get_dataset_path(dataset):\n",
    "    base_path = f\"../../train_test_splits/\"\n",
    "    \n",
    "    train_path = os.path.join(base_path, f\"{dataset}_train.txt\")\n",
    "    val_path = os.path.join(base_path, f\"{dataset}_val.txt\")\n",
    "    \n",
    "    return train_path, val_path\n",
    "\n",
    "\n",
    "def get_transforms(dataset, return_ctx=False):\n",
    "    f = open('../../dataset_config.json')\n",
    "    ctx = json.load(f)[dataset]\n",
    "\n",
    "    transform_data = T.Normalize(ctx['data_mean'], ctx['data_std'])\n",
    "    transform_label = T.MinMaxNormalize(ctx['label_min'], ctx['label_max'])\n",
    "    if return_ctx:\n",
    "        return  transform_data, transform_label, ctx\n",
    "    return  transform_data, transform_label\n",
    "\n",
    "\n",
    "\n",
    "def get_transforms_vel_gan(dataset, return_ctx=False):\n",
    "    k=1\n",
    "    f = open('../../dataset_config.json')\n",
    "    ctx = json.load(f)[dataset]\n",
    "    \n",
    "    # Normalize data and label to [-1, 1]\n",
    "    transform_data = Compose([\n",
    "                            T.LogTransform(k=k),\n",
    "                            T.MinMaxNormalize(T.log_transform(ctx['data_min'], k=k), \n",
    "                            T.log_transform(ctx['data_max'], k=k))\n",
    "                        ])\n",
    "    transform_label = T.MinMaxNormalize(ctx['label_min'], ctx['label_max'])\n",
    "    \n",
    "    if return_ctx:\n",
    "        return  transform_data, transform_label, ctx\n",
    "    return  transform_data, transform_label\n",
    "\n",
    "def get_transforms_auto_linear(dataset, return_ctx=False):\n",
    "    f = open('../../dataset_config.json')\n",
    "    ctx = json.load(f)[dataset]\n",
    "    \n",
    "    transform_data = Compose([\n",
    "                        T.LogTransform(k=k),\n",
    "                        T.MinMaxNormalize(T.log_transform(ctx['data_min'], k=k), \n",
    "                        T.log_transform(ctx['data_max'], k=k))\n",
    "                    ])\n",
    "\n",
    "    transform_label = T.MinMaxNormalize(ctx['label_min'], ctx['label_max'])\n",
    "\n",
    "    inverse_transform_data = Compose([\n",
    "        T.MinMaxNormalize(T.log_transform(ctx['data_min'], k=k), \n",
    "                            T.log_transform(ctx['data_max'], k=k)).inverse_transform,\n",
    "        T.LogTransform(k=k).inverse_transform\n",
    "    ])\n",
    "\n",
    "    min_log = T.log_transform(ctx['data_min'], k)\n",
    "    max_log = T.log_transform(ctx['data_max'],k)\n",
    "    if return_ctx:\n",
    "        return  transform_data, transform_label, ctx\n",
    "    return  transform_data, transform_label, min_log, max_log\n",
    "\n",
    "\n",
    "def get_dataloader(dataset, transform_mode=\"normal\", train_dataset=None):\n",
    "    \n",
    "    if train_dataset is None:\n",
    "        train_dataset = dataset\n",
    "        \n",
    "    if transform_mode ==\"normal\":    \n",
    "        transform_data, transform_label, ctx = get_transforms(train_dataset, return_ctx=True)\n",
    "    if transform_mode ==\"VelocityGAN\":\n",
    "        transform_data, transform_label, ctx = get_transforms_vel_gan(train_dataset, return_ctx=True)\n",
    "    if transform_mode ==\"AutoLinear\":\n",
    "        transform_data, transform_label, ctx = get_transforms_auto_linear(train_dataset, return_ctx=True)\n",
    "\n",
    "    train_anno, val_anno = get_dataset_path(dataset)\n",
    "        \n",
    "    print(f'Loading {dataset} validation data')\n",
    "    dataset_valid = FWIDataset(\n",
    "        val_anno,\n",
    "        preload=True,\n",
    "        sample_ratio=sample_temporal,\n",
    "        file_size=ctx['file_size'],\n",
    "        transform_data=transform_data,\n",
    "        transform_label=transform_label\n",
    "    )\n",
    "        \n",
    "    valid_sampler = RandomSampler(dataset_valid)\n",
    "\n",
    "    dataloader_valid = DataLoader(\n",
    "                                dataset_valid, batch_size=batch_size,\n",
    "                                sampler=valid_sampler, num_workers=workers,\n",
    "                                pin_memory=True, collate_fn=default_collate, shuffle=False)\n",
    "    \n",
    "    print('Data loading over')\n",
    "        \n",
    "    return dataset_valid, dataloader_valid, transform_data, transform_label "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_inverse_params(inverse_model_params, model_path=None):\n",
    "        inverse_model_params.setdefault('IUnetInverseModel', {})\n",
    "        inverse_model_params['IUnetInverseModel']['cfg_path'] = cfg_path\n",
    "        inverse_model_params['IUnetInverseModel']['latent_dim'] = latent_dim\n",
    "        \n",
    "        inverse_model_params.setdefault('UNetInverseModel', {})\n",
    "        inverse_model_params['UNetInverseModel']['cfg_path'] = cfg_path\n",
    "        inverse_model_params['UNetInverseModel']['latent_dim'] = latent_dim\n",
    "        if \"UNetInverseModel_17M\" in model_path:\n",
    "            print(\"here\")\n",
    "            inverse_model_params['UNetInverseModel']['unet_depth'] = architecture_params[\"UNetInverseModel_17M\"][\"unet_depth\"]\n",
    "            inverse_model_params['UNetInverseModel']['unet_repeat_blocks'] = architecture_params[\"UNetInverseModel_17M\"][\"unet_repeat_blocks\"]\n",
    "        else:\n",
    "            inverse_model_params['UNetInverseModel']['unet_depth'] = architecture_params[\"default\"][\"unet_depth\"]\n",
    "            inverse_model_params['UNetInverseModel']['unet_repeat_blocks'] = architecture_params[\"default\"][\"unet_repeat_blocks\"]\n",
    "            \n",
    "        inverse_model_params['UNetInverseModel']['skip'] = skip # skip true\n",
    "        return inverse_model_params\n",
    "    \n",
    "    \n",
    "def get_model(model_path, model_type):\n",
    "    try:\n",
    "        print(model_path, model_type)\n",
    "        inverse_model_params = inverse_network.inverse_params\n",
    "        inverse_model_params = set_inverse_params(inverse_model_params, model_path)\n",
    "        model = inverse_network.model_dict[model_type](**inverse_model_params[model_type]).to(device)\n",
    "        checkpoint = torch.load(model_path, map_location=torch.device('cpu'))\n",
    "        model.load_state_dict(checkpoint['model'])\n",
    "    except:\n",
    "        print(\"Failed to load new model. Falling back to Legacy Code.\")\n",
    "        inverse_model_params = inverse_network.inverse_params_legacy\n",
    "        if \"UNetInverseModel_17M\" in model_path:\n",
    "            inverse_model_params['unet_depth'] = architecture_params[\"UNetInverseModel_17M\"][\"unet_depth\"]\n",
    "            inverse_model_params['unet_repeat_blocks'] = architecture_params[\"UNetInverseModel_17M\"][\"unet_repeat_blocks\"]\n",
    "        else:\n",
    "            inverse_model_params['unet_depth'] = architecture_params[\"default\"][\"unet_depth\"]\n",
    "            inverse_model_params['unet_repeat_blocks'] = architecture_params[\"default\"][\"unet_repeat_blocks\"]\n",
    "            \n",
    "        model_type = model_type+\"_Legacy\"\n",
    "        model = inverse_network.model_dict[model_type](**inverse_model_params).to(device)\n",
    "        checkpoint = torch.load(model_path, map_location=torch.device('cpu'))\n",
    "        model.load_state_dict(checkpoint['model'])\n",
    "\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    return model\n",
    "\n",
    "def get_model_iunet_(amp_model, vel_model, latent_channels, model_type):\n",
    "    if model_type == \"IUNET\":\n",
    "        iunet_model = iUNet(in_channels=latent_channels, dim=2, architecture=(4,4,4,4))\n",
    "        model = iunet_network.IUnetModel(amp_model, vel_model, iunet_model).to(device)\n",
    "        print(\"IUnet model initialized.\")\n",
    "    elif model_type == \"Decouple_IUnet\":\n",
    "        amp_iunet_model = iUNet(in_channels=latent_channels, dim=2, architecture=(4,4,4,4))\n",
    "        vel_iunet_model = iUNet(in_channels=latent_channels, dim=2, architecture=(4,4,4,4))\n",
    "        model = iunet_network.Decouple_IUnetModel(amp_model, vel_model, amp_iunet_model, vel_iunet_model).to(device)\n",
    "        print(\"Decoupled IUnetModel model initialized.\")\n",
    "    else:\n",
    "        print(f\"Invalid Model: {model_type}\")\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_model_iunet(model_path, model_type):\n",
    "    try:   \n",
    "        print(model_path, model_type)\n",
    "        amp_cfg_name = get_config_name(latent_dim, model_type=\"amplitude\")\n",
    "        amp_model = autoencoder.AutoEncoder(cfg_path, amp_cfg_name).to(device)\n",
    "\n",
    "        # creating velocity cnn\n",
    "        vel_cfg_name = get_config_name(latent_dim, model_type=\"velocity\")\n",
    "        vel_model = autoencoder.AutoEncoder(cfg_path, vel_cfg_name).to(device)\n",
    "\n",
    "        latent_channels = get_latent_dim(cfg_path, amp_cfg_name)\n",
    "        model = get_model_iunet_(amp_model, vel_model, latent_channels, model_type)\n",
    "        checkpoint = torch.load(model_path, map_location=torch.device('cpu'))\n",
    "        model.load_state_dict(checkpoint['model'])\n",
    "        \n",
    "    except:\n",
    "        print(\"Failed to load new model. Falling back to Legacy Code.\")\n",
    "        amp_input_channel = 5\n",
    "        amp_encoder_channel = [8, 16, 32, 64, 128]\n",
    "        amp_decoder_channel = [128, 64, 32, 16, 5]\n",
    "        amp_model = iunet_network.AmpAutoEncoder(amp_input_channel, amp_encoder_channel, amp_decoder_channel).to(device)\n",
    "\n",
    "        # creating velocity cnn\n",
    "        vel_input_channel = 1\n",
    "        vel_encoder_channel = [8, 16, 32, 64, 128]\n",
    "        vel_decoder_channel = [128, 64, 32, 16, 1]\n",
    "        vel_model = iunet_network.VelAutoEncoder(vel_input_channel, vel_encoder_channel, vel_decoder_channel).to(device)\n",
    "\n",
    "        latent_channels = 128\n",
    "        model = get_model_iunet_(amp_model, vel_model, latent_channels, model_type)\n",
    "        checkpoint = torch.load(model_path, map_location=torch.device('cpu'))\n",
    "        model.load_state_dict(checkpoint['model'])\n",
    "\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def get_model_auto_linear(model_path, model_type):\n",
    "    # Load the TorchScript model\n",
    "    model = torch.jit.load(model_path)\n",
    "\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def permute_fn(tensor):\n",
    "    return tensor\n",
    "\n",
    "\n",
    "def plot_instance(amp_true, vel_true, transform_label, \n",
    "                  amp_true_vel_gan, vel_true_vel_gan, transform_label_vel_gan, \n",
    "                  amp_true_auto_linear, vel_true_auto_linear, transform_label_auto_linear,\n",
    "                  model_paths, model_train_dataset=\"FlatVel-A\", evaluate_dataset=\"flatvel-b\"):\n",
    "    values_dict = {}\n",
    "    \n",
    "    vel_true_np = transform_label.inverse_transform(permute_fn(vel_true).detach().cpu().numpy())\n",
    "    values_dict[\"ground_truth\"] = vel_true_np\n",
    "    \n",
    "    for idx, architecture_name in enumerate(architecture_names):\n",
    "        print(\"Evaluating Architecture: \", architecture_name)\n",
    "\n",
    "        model_path = os.path.join(base_path,model_paths[model_train_dataset][architecture_name],\"latest_checkpoint.pth\")\n",
    "        print(\"Evaluating Model: \", model_path)\n",
    "        #load model\n",
    "        if architecture_types[idx] == \"IUNET\":\n",
    "            model = get_model_iunet(model_path, architecture_types[idx])\n",
    "            model = model.to(device)\n",
    "            vel_pred = model.inverse(amp_true)  \n",
    "        elif architecture_types[idx] == \"AutoLinearInverse\":\n",
    "            model = get_model_auto_linear(model_path, architecture_types[idx])\n",
    "            model = model.to(device)\n",
    "            output = model(amp_true_auto_linear, vel_true_auto_linear)\n",
    "            vel_pred_auto_linear = output[4]\n",
    "        else:\n",
    "            model = get_model(model_path, architecture_types[idx])  \n",
    "            model = model.to(device)\n",
    "            if architecture_name == \"Velocity_GAN\":\n",
    "                vel_pred_vel_gan = model(amp_true_vel_gan)\n",
    "            else:\n",
    "                vel_pred = model(amp_true)\n",
    "        \n",
    "         ### diff normalization so diff inverse transofrm, not doing for inversion net as we retrain it with right normalization\n",
    "        if architecture_name == \"Velocity_GAN\":\n",
    "            vel_pred_np = transform_label_vel_gan.inverse_transform(permute_fn(vel_pred_vel_gan).detach().cpu().numpy())\n",
    "        elif architecture_types[idx] == \"AutoLinearInverse\":\n",
    "            vel_pred_np = transform_label_auto_linear.inverse_transform(permute_fn(vel_pred_auto_linear).detach().cpu().numpy())\n",
    "        else:\n",
    "            vel_pred_np = transform_label.inverse_transform(permute_fn(vel_pred).detach().cpu().numpy())\n",
    "        values_dict[architecture_name] = vel_pred_np\n",
    "        \n",
    "\n",
    "    ## viz path for vizualizing all models in one place\n",
    "    vis_path = os.path.join(base_path, model_train_dataset, viz_save_path)\n",
    "    if not os.path.exists(vis_path):\n",
    "        os.makedirs(vis_path) \n",
    "    ################\n",
    "    generate_plot_viz(values_dict, vis_path, evaluate_dataset, num_images)\n",
    "    return values_dict\n",
    "\n",
    "\n",
    "from copy import deepcopy\n",
    "def add_diff_plots(values_dict):\n",
    "    values_dict_new = deepcopy(values_dict)\n",
    "    ground_truth = values_dict[\"ground_truth\"]\n",
    "    for key in values_dict.keys():\n",
    "        if key!=\"ground_truth\":\n",
    "            diff = values_dict[\"ground_truth\"] - values_dict[key]\n",
    "            values_dict_new[f\"diff_{key}\"] = diff\n",
    "    values_dict_new[\"XNET_cyle-InversionNet\"] = values_dict_new[\"Invertible_XNet_cycle_warmup\"] - values_dict_new[\"InversionNet\"]\n",
    "    values_dict_new[\"XNET_cyle-XNET\"] = values_dict_new[\"Invertible_XNet_cycle_warmup\"] - values_dict_new[\"Invertible_XNet\"]\n",
    "    return values_dict_new\n",
    "\n",
    "\n",
    "def generate_plot_viz(values_dict, vis_path, evaluate_dataset, num_images, plot=True):\n",
    "    save_name = f\"{evaluate_dataset}_{mode}\"\n",
    "    num_cols = len(values_dict.keys())\n",
    "    \n",
    "#     fig, axes = plt.subplots(num_images, num_cols, figsize=(3.5*num_cols, int(3*num_images)), dpi=150)\n",
    "    fig = plt.figure(figsize=(3.3*num_cols, int(3.3*num_images)), dpi=150)\n",
    "    gs = gridspec.GridSpec(num_images, num_cols + 1, width_ratios=[1]*num_cols + [0.1])\n",
    "    \n",
    "    for i in range(num_images):\n",
    "        vel_min, vel_max = np.inf, -np.inf\n",
    "        for key, values in values_dict.items():\n",
    "            vel_min = min(vel_min, values[i].min())\n",
    "            vel_max = max(vel_max, values[i].max())\n",
    "            \n",
    "        for j, (key, values) in enumerate(values_dict.items()):\n",
    "#             ax = axes[i, j]\n",
    "            ax = fig.add_subplot(gs[i, j])\n",
    "            img = ax.imshow(values_dict[key][i, 0], aspect='auto', vmin=vel_min, vmax=vel_max, cmap=rainbow_cmap)\n",
    "#             divider = make_axes_locatable(ax)\n",
    "#             cax = divider.append_axes(\"right\", size=\"10%\", pad=0.05)\n",
    "#             plt.colorbar(img, cax=cax)\n",
    "            plot_name = plot_names[key] if key in plot_names else key\n",
    "            ax.set_title(f\"{plot_name}\", fontsize=font_sizes[\"sub_plt_title\"])\n",
    "#             ax.set_title(f\"{key}: Image {i}\", fontsize=12)\n",
    "            if j==0:\n",
    "                ax.set_ylabel(plot_dataset_names[f\"{evaluate_dataset}_{i}\"], fontsize=font_sizes[\"dataset_name\"])\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([]) \n",
    "            \n",
    "        divider = make_axes_locatable(ax)\n",
    "        cax = fig.add_subplot(gs[i, num_cols])\n",
    "        cbar = fig.colorbar(img, cax=cax)\n",
    "        cbar.ax.tick_params(labelsize=font_sizes[\"color_bar\"]) \n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(vis_path, f\"{save_name}.pdf\"))\n",
    "    if plot:\n",
    "        plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    \n",
    "def generate_plot_trace(values_dict, vis_path, evaluate_dataset, num_images, plot=True):\n",
    "    if not os.path.exists(vis_path):\n",
    "        os.makedirs(vis_path) \n",
    "    values_dict = {key: value for key, value in values_dict.items() if key in plot_map}\n",
    "    save_name = evaluate_dataset\n",
    "    horizontal_indices = [17, 34, 51]\n",
    "    vertical_indices = [17, 34, 51]\n",
    "    for i in range(num_images):\n",
    "        plot_trace_plots(values_dict, horizontal_indices, direction='horizontal', image_id=i, vis_path=vis_path, save_name=save_name)\n",
    "        plot_trace_plots(values_dict, vertical_indices, direction='vertical', image_id=i, vis_path=vis_path, save_name=save_name)\n",
    "\n",
    "    \n",
    "def plot_trace_plots(values_dict, indices, direction='horizontal', image_id=0, vis_path=\"\", save_name=\"\"):\n",
    "    \"\"\"\n",
    "    Plot trace plots for the given indices.\n",
    "\n",
    "    Parameters:\n",
    "    values_dict (dictionary): The ground truth matrix with shape (h, w).\n",
    "    indices (list of int): The indices at which to plot the trace plots.\n",
    "    direction (str): The direction of the trace plot ('horizontal' or 'vertical').\n",
    "    \"\"\"\n",
    "    \n",
    "    # Determine the number of plots\n",
    "    num_trace_plots = len(indices)\n",
    "    num_comparison_plots = len(values_dict)\n",
    "    \n",
    "    # Plot the original u_gt, u_pred and the difference\n",
    "#     fig, axes = plt.subplots(1, num_comparison_plots, figsize=(3*num_comparison_plots, 2.7))\n",
    "    fig = plt.figure(figsize=(3*num_comparison_plots, 3))\n",
    "    gs = gridspec.GridSpec(1, num_comparison_plots + 1, width_ratios=[1]*num_comparison_plots + [0.1])\n",
    "    \n",
    "    for i, (key, values) in enumerate(values_dict.items()):\n",
    "#         ax = axes[i]\n",
    "        ax = fig.add_subplot(gs[i])\n",
    "        im = ax.imshow(values[image_id, 0], aspect='auto', cmap=rainbow_cmap)\n",
    "#         plt.colorbar(im, ax=ax)\n",
    "        ax.set_title(f'{plot_names[key]}')\n",
    "        for idx in indices:\n",
    "            ax.axhline(y=idx, color='white', linestyle='--') if direction == 'horizontal' else ax.axvline(x=idx, color='white', linestyle='--')\n",
    "        xlabels = \"Sensors Locations (m)\"\n",
    "        ax.set_xlabel(xlabels)\n",
    "        ylabels = \"Depth (m)\"\n",
    "        if i==0:\n",
    "            ax.set_ylabel(ylabels)\n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = fig.add_subplot(gs[num_comparison_plots])\n",
    "    cbar = fig.colorbar(im, cax=cax)\n",
    "    cbar.ax.tick_params(labelsize=font_sizes[\"color_bar\"])    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(vis_path, f\"{save_name}_Image{image_id}_{direction}_compare.pdf\"))\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "    # Set up the figure with subplots in a N x 1 configuration\n",
    "    fig, axes = plt.subplots(1, num_trace_plots, figsize=(3 * num_trace_plots, 2.7))\n",
    "    \n",
    "    # If there's only one index, axes will not be an array, so we wrap it in a list for consistency\n",
    "    if num_trace_plots == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    # Loop over the indices and plot the trace plots\n",
    "    for i, idx in enumerate(indices):\n",
    "        for j, (key, values) in enumerate(values_dict.items()):\n",
    "            slice_ = values[image_id, 0, idx, :] if direction == 'horizontal' else values[image_id, 0, :, idx]\n",
    "#             linestyle = '--' if key==\"ground_truth\" else '-'\n",
    "#             axes[i].plot(slice_, linestyle=linestyle, label=f'{key}')\n",
    "            linestyle = plot_map[key][\"linestyle\"]\n",
    "            color = plot_map[key][\"color\"]\n",
    "            zorder = plot_map[key][\"zorder\"]\n",
    "            axes[i].plot(slice_, linestyle=linestyle, color=color, zorder=zorder, label=f'{plot_names[key]}')\n",
    "                \n",
    "        # Set labels and title\n",
    "        axes[i].set_ylabel(f\"Trace at index {idx} {direction}\")\n",
    "        xlabels = \"Sensors Locations\" if direction==\"horizontal\" else \"Depth\"\n",
    "        axes[i].set_xlabel(xlabels)\n",
    "#         axes[i].legend(loc='upper right', fontsize=8)\n",
    "    \n",
    "    handles, labels = axes[0].get_legend_handles_labels()\n",
    "\n",
    "    fig.legend(handles, labels, bbox_to_anchor=(0.5, 0.05),loc='upper center', ncol=4)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(vis_path, f\"{save_name}_Image{image_id}_{direction}_TracePlot.pdf\"), \n",
    "                             bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "    \n",
    "def do_everything(model_train_datasets, evaluate_datasets, model_paths):\n",
    "    items_dict = {}\n",
    "    for idx, evaluate_dataset in enumerate(evaluate_datasets):\n",
    "        print(\"Target Dataset: \", evaluate_dataset)\n",
    "        \n",
    "        dataset_val, _, transform_data, transform_label = get_dataloader(evaluate_dataset)\n",
    "        items = np.random.choice(len(dataset_val), num_images)\n",
    "        items_dict[evaluate_dataset] = items\n",
    "\n",
    "        _, amp_true, vel_true = dataset_val[items]\n",
    "        amp_true, vel_true = torch.tensor(amp_true).to(device), torch.tensor(vel_true).to(device)\n",
    "        \n",
    "        #### diff normalization for vel gan\n",
    "        dataset_val_vel_gan, _, transform_data_vel_gan, transform_label_vel_gan = get_dataloader(evaluate_dataset, transform_mode = \"VelocityGAN\")  \n",
    "        _, amp_true_vel_gan, vel_true_vel_gan = dataset_val_vel_gan[items]\n",
    "        amp_true_vel_gan, vel_true_vel_gan = torch.tensor(amp_true_vel_gan).to(device), torch.tensor(vel_true_vel_gan).to(device)\n",
    "        \n",
    "         \n",
    "        #### diff normalization for auto linear\n",
    "        dataset_val_auto_linear, _, transform_data_auto_linear, transform_label_auto_linear = get_dataloader(evaluate_dataset, transform_mode = \"AutoLinear\")  \n",
    "        _, amp_true_auto_linear, vel_true_auto_linear = dataset_val_auto_linear[items]\n",
    "        amp_true_auto_linear, vel_true_auto_linear = torch.tensor(amp_true_auto_linear).to(device), torch.tensor(vel_true_auto_linear).to(device)\n",
    "        \n",
    "        \n",
    "        \n",
    "        for model_train_dataset in [model_train_datasets[idx]]:\n",
    "            print(\"Source Dataset: \", model_train_dataset)\n",
    "            value_dict = plot_instance(amp_true, vel_true, transform_label, \n",
    "                                       amp_true_vel_gan, vel_true_vel_gan, transform_label_vel_gan, \n",
    "                                       amp_true_auto_linear, vel_true_auto_linear, transform_label_auto_linear,\n",
    "                                       model_paths, model_train_dataset, evaluate_dataset)\n",
    "    return value_dict,items_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_marmousi(evaluate_dataset=\"marmousi\", model_train_dataset=\"Style-A\",model_paths=model_paths):\n",
    "    \n",
    "    model_train_dataset_ = model_train_dataset.lower()\n",
    "    dataset_val, _, transform_data, transform_label = get_dataloader(dataset=evaluate_dataset, \n",
    "                                                                     train_dataset=model_train_dataset_)\n",
    "    items = np.array([0, 1]) # marmousi dataset has 2 items\n",
    "    _, amp_true, vel_true = dataset_val[items]\n",
    "    amp_true, vel_true = torch.tensor(amp_true).to(device), torch.tensor(vel_true).to(device)\n",
    "    \n",
    "    #### diff normalization for vel gan\n",
    "    dataset_val_vel_gan, _, transform_data_vel_gan, transform_label_vel_gan = get_dataloader(evaluate_dataset, train_dataset=model_train_dataset_, transform_mode = \"VelocityGAN\")  \n",
    "    _, amp_true_vel_gan, vel_true_vel_gan = dataset_val_vel_gan[items]\n",
    "    amp_true_vel_gan, vel_true_vel_gan = torch.tensor(amp_true_vel_gan).to(device), torch.tensor(vel_true_vel_gan).to(device)\n",
    "\n",
    "\n",
    "    #### diff normalization for auto linear\n",
    "    dataset_val_auto_linear, _, transform_data_auto_linear, transform_label_auto_linear = get_dataloader(evaluate_dataset, train_dataset=model_train_dataset_, transform_mode = \"AutoLinear\")  \n",
    "    _, amp_true_auto_linear, vel_true_auto_linear = dataset_val_auto_linear[items]\n",
    "    amp_true_auto_linear, vel_true_auto_linear = torch.tensor(amp_true_auto_linear).to(device), torch.tensor(vel_true_auto_linear).to(device)\n",
    "        \n",
    "        \n",
    "    print(\"Source Dataset: \", model_train_dataset)\n",
    "    value_dict = plot_instance(amp_true, vel_true, transform_label, \n",
    "                                       amp_true_vel_gan, vel_true_vel_gan, transform_label_vel_gan, \n",
    "                                       amp_true_auto_linear, vel_true_auto_linear, transform_label_auto_linear,\n",
    "                                       model_paths, model_train_dataset, evaluate_dataset)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_train_dataset in model_train_datasets:\n",
    "    for evaluate_dataset in evaluate_datasets:\n",
    "        evaluate_marmousi(evaluate_dataset=evaluate_dataset, model_train_dataset=model_train_dataset, model_paths=model_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openfwi_env",
   "language": "python",
   "name": "openfwi_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
