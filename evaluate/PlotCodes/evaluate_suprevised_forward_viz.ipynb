{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "print(sys.path)\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "import openpyxl\n",
    "from openpyxl import Workbook\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import TwoSlopeNorm\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torchvision.transforms import Compose\n",
    "from torch.utils.data import RandomSampler, DataLoader\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.nn.parallel import DistributedDataParallel\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from iunets import iUNet\n",
    "from dataset import FWIDataset\n",
    "# from networks import iunet_network\n",
    "from networks import forward_network, inverse_network, iunet_network\n",
    "\n",
    "import utils.transforms as T\n",
    "from utils.pytorch_ssim import *\n",
    "import utils.utilities as utils\n",
    "from utils.scheduler import WarmupMultiStepLR\n",
    "from matplotlib.colors import ListedColormap\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "\n",
    "rainbow_cmap = ListedColormap(np.load('/projects/ml4science/OpenFWI/Latent_Bijectivity/utils/rainbow256.npy'))\n",
    "\n",
    "forward_model_list = [forward_network.FNO2d, forward_network.WaveformNet, forward_network.WaveformNet_V2, iunet_network.IUnetForwardModel, iunet_network.UNetForwardModel]\n",
    "inverse_model_list = [inverse_network.InversionNet, iunet_network.IUnetInverseModel, iunet_network.UNetInverseModel]\n",
    "joint_model_list = [iunet_network.IUnetModel, iunet_network.JointModel, iunet_network.Decouple_IUnetModel]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 0\n",
    "file_size = 500\n",
    "vis_suffix = False\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "k = 1\n",
    "workers = 4\n",
    "lambda_g1v = 1\n",
    "lambda_g2v = 1\n",
    "batch_size = 50\n",
    "mask_factor = 0.0\n",
    "sample_temporal = 1\n",
    "distributed = False\n",
    "\n",
    "num_images = 20\n",
    "skip = 0\n",
    "cfg_path = \"../../configs/\"\n",
    "latent_dim = 70\n",
    "\n",
    "\n",
    "\n",
    "base_path = \"/projects/ml4science/OpenFWI/Results/SupervisedExperiment/\"\n",
    "mode = \"main_paper\" #\"appendix\" or \"main_paper\"\n",
    "viz_save_path = f'AAAI_viz_aug15/{mode}'\n",
    "font_sizes = {\"color_bar\": 16, \"sub_plt_title\":20, \"dataset_name\": 20}\n",
    "\n",
    "#Runs in supervised fashion, models[i] on evaluate[i]\n",
    "evaluate_datasets = [\"flatvel-a\", \"flatvel-b\", \"curvevel-a\", \"curvevel-b\",\n",
    "                     \"flatfault-a\", \"flatfault-b\",  \"curvefault-a\", \"curvefault-b\", \"style-a\", \"style-b\"]\n",
    "\n",
    "model_train_datasets = ['FlatVel-A', 'FlatVel-B', 'CurveVel-A', 'CurveVel-B',\n",
    "                        'FlatFault-A', 'FlatFault-B', 'CurveFault-A', 'CurveFault-B', 'Style-A', 'Style-B']\n",
    "\n",
    "if mode == \"appendix\":\n",
    "    architecture_types =  [\"FNO\",  \"WaveformNet\", \"AutoLinearForward\", \"UNetForwardModel\", \"UNetForwardModel\",  \n",
    "                      \"IUNET\", \"IUNET\"]\n",
    "                     \n",
    "    architecture_names =  [\"FNO\", \"WaveformNet\", \"AutoLinear_Forward_ckpt\", \"UNetForwardModel_17M\", \"UNetForwardModel_33M\", \n",
    "                     \"Invertible_XNet\", \"Invertible_XNet_cycle_warmup\"]\n",
    "else:\n",
    "\n",
    "    architecture_types =  [\"FNO\", \"AutoLinearForward\", \"UNetForwardModel\", \"IUNET\"]\n",
    "\n",
    "    architecture_names =  [\"FNO\", \"AutoLinear_Forward_ckpt\", \"UNetForwardModel_33M\", \"Invertible_XNet\"]\n",
    "    \n",
    "\n",
    "\n",
    "plot_names = {\"FNO\": \"FNO\", \"UNetForwardModel_17M\": \"Latent U-Net (Small)\", \n",
    "              \"UNetForwardModel_33M\": \"Latent U-Net (Large)\",\n",
    "              \"IUnetForwardModel\": \"IUnetForwardModel\" ,\n",
    "              \"Invertible_XNet\": \"Invertible X-Net\", \"Invertible_XNet_cycle_warmup\": \"Invertible X-Net (Cycle)\",\n",
    "             \"Velocity_GAN\": \"VelocityGAN\", \"WaveformNet\": \"WaveformNet\", \"ground_truth\": \"Ground Truth\",\n",
    "             \"AutoLinear_Forward_ckpt\": \"Auto-Linear\"}\n",
    "\n",
    "plot_dataset_names = {\"flatvel-a\": \"FVA\", \"flatvel-b\": \"FVB\", \"curvevel-a\": \"CVA\", \"curvevel-b\": \"CVB\",\n",
    "                     \"flatfault-a\": \"FFA\", \"flatfault-b\": \"FFB\",  \"curvefault-a\": \"CFA\", \"curvefault-b\": \"CFB\", \n",
    "                      \"style-a\": \"STA\", \"style-b\": \"STB\"}\n",
    "\n",
    "architecture_params = {\"UNetForwardModel_17M\":{\"unet_depth\": 2, \"unet_repeat_blocks\": 1}, \n",
    "                       \"UNetForwardModel_33M\":{\"unet_depth\": 2, \"unet_repeat_blocks\": 2},\n",
    "                       \"default\":{\"unet_depth\": 2, \"unet_repeat_blocks\": 2}\n",
    "                      }\n",
    "model_paths = {}\n",
    "for model_name in model_train_datasets:\n",
    "    model_paths[model_name] = {}\n",
    "    for i, architecture_name in enumerate(architecture_names):\n",
    "        path_ = os.path.join(model_name, architecture_name, \"fcn_l1loss_ffb\")\n",
    "        model_paths[model_name][architecture_name]= path_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "linestyle_tuple = [\n",
    "     ('loosely dotted',        (0, (1, 10))),\n",
    "     ('dotted',                (0, (1, 1))),\n",
    "     ('densely dotted',        (0, (1, 1))),\n",
    "     ('long dash with offset', (5, (10, 3))),\n",
    "     ('loosely dashed',        (0, (5, 10))),\n",
    "     ('dashed',                (0, (5, 5))),\n",
    "     ('densely dashed',        (0, (5, 1))),\n",
    "\n",
    "     ('loosely dashdotted',    (0, (3, 10, 1, 10))),\n",
    "     ('dashdotted',            (0, (3, 5, 1, 5))),\n",
    "     ('densely dashdotted',    (0, (3, 1, 1, 1))),\n",
    "\n",
    "     ('dashdotdotted',         (0, (3, 5, 1, 5, 1, 5))),\n",
    "     ('loosely dashdotdotted', (0, (3, 10, 1, 10, 1, 10))),\n",
    "     ('densely dashdotdotted', (0, (3, 1, 1, 1, 1, 1)))]\n",
    "\n",
    "linestyle_dict = {name: style for name, style in linestyle_tuple}\n",
    "\n",
    "plot_map = {\n",
    "                \"ground_truth\":{\n",
    "                   \"linestyle\":\"solid\", \n",
    "                   \"color\":\"black\",\n",
    "                   \"zorder\":0,\n",
    "                  },\n",
    "                 \"FNO\":{\n",
    "                   \"linestyle\":linestyle_dict[\"dashed\"], \n",
    "                   \"color\":\"blue\",\n",
    "                   \"zorder\":3,\n",
    "                  },\n",
    "                 \"UNetForwardModel_33M\":{\n",
    "                   \"linestyle\":linestyle_dict[\"densely dashed\"], \n",
    "                   \"color\":\"red\",\n",
    "                   \"zorder\":2,\n",
    "                  },\n",
    "                  \"Invertible_XNet\":{\n",
    "                   \"linestyle\":\"dotted\", \n",
    "                   \"color\":\"darkgreen\",\n",
    "                   \"zorder\":1,\n",
    "                  },\n",
    "           }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_items_dict = {'flatvel-a': np.array([1488, 2253,  819, 1046, 3920,  633,  964, 5898, 5933, 4383,  315,\n",
    "         256, 5837, 4492,  389, 2186, 5272, 4972, 4383, 3516]),\n",
    " 'flatvel-b': np.array([ 974, 3454, 5279,  894, 2401, 5405,  114, 5189, 3564, 3560, 4970,\n",
    "        2284, 1872, 3680,  331,   94, 5782, 4302, 5120,  569]),\n",
    " 'curvevel-a': np.array([1441, 2121, 2343, 2331, 3659, 4457, 2403, 1546, 3575, 1234, 2294,\n",
    "         255, 5613, 3010, 3647, 4285, 3442,  475, 4226,  942]),\n",
    " 'curvevel-b': np.array([3417,  332, 5871, 3862,  200, 3372, 5210, 3169, 5479,  751, 3873,\n",
    "        5162, 5260, 2470, 4395,  711, 1204, 3886, 2100, 3956]),\n",
    " 'flatfault-a': np.array([1492, 3869, 5428, 2941, 4577, 5167, 5313, 1357, 4488, 2116, 4468,\n",
    "        4232, 1402,  349,  801,   93, 5210, 1984,  640, 5807]),\n",
    " 'flatfault-b': np.array([2200, 2507, 2531, 1484,  163,   87, 1756, 1864, 4972, 3109, 2258,\n",
    "        4510, 5983, 5145, 4854, 1234, 2423,  696, 4337, 2290]),\n",
    " 'curvefault-a': np.array([5458, 3833, 4271, 1892, 5448, 1351, 2580, 2004, 2699, 3761, 3630,\n",
    "        1238,  300, 5753, 4806, 3049, 1157, 2890, 4465,  144]),\n",
    " 'curvefault-b': np.array([4998,  580, 3749, 2775, 5798, 2342,   24, 3622, 2406,  443, 3561,\n",
    "        5511, 4919, 2228, 4075, 1330, 4050, 1068,  263, 1840]),\n",
    " 'style-a': np.array([ 121, 3384, 1096, 5874,  312, 4093, 6969, 6105, 4649, 6392, 3347,\n",
    "        2649, 4184, 4634,   96, 5287, 2963, 4879, 3525,  684]),\n",
    " 'style-b': np.array([1771,  508, 6559,  386, 1349, 4491, 3294,  131, 6602, 6553, 3832,\n",
    "        2119,  880, 4273, 5477, 5846, 2790,  773, 3311,  416])}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterions = {\n",
    "    'MAE': lambda x, y: torch.mean(torch.abs(x - y)),\n",
    "    'MSE': lambda x, y: torch.mean((x - y) ** 2)\n",
    "}\n",
    "\n",
    "def get_dataset_path(dataset):\n",
    "    arr = dataset.split(\"-\")\n",
    "    base_path = f\"../../train_test_splits/\"\n",
    "    \n",
    "    train_path = os.path.join(base_path, f\"{arr[0]}_{arr[1]}_train.txt\")\n",
    "    val_path = os.path.join(base_path, f\"{arr[0]}_{arr[1]}_val.txt\")\n",
    "    \n",
    "    return train_path, val_path\n",
    "\n",
    "def get_transforms(dataset, return_ctx=False):\n",
    "    f = open('../../dataset_config.json')\n",
    "    ctx = json.load(f)[dataset]\n",
    "\n",
    "    transform_data = T.Normalize(ctx['data_mean'], ctx['data_std'])\n",
    "    transform_label = T.MinMaxNormalize(ctx['label_min'], ctx['label_max'])\n",
    "    if return_ctx:\n",
    "        return  transform_data, transform_label, ctx\n",
    "    return  transform_data, transform_label\n",
    "\n",
    "def get_transforms_auto_linear(dataset, return_ctx=False):\n",
    "    f = open('../../dataset_config.json')\n",
    "    ctx = json.load(f)[dataset]\n",
    "    \n",
    "    transform_data = Compose([\n",
    "                        T.LogTransform(k=k),\n",
    "                        T.MinMaxNormalize(T.log_transform(ctx['data_min'], k=k), \n",
    "                        T.log_transform(ctx['data_max'], k=k))\n",
    "                    ])\n",
    "\n",
    "    transform_label = T.MinMaxNormalize(ctx['label_min'], ctx['label_max'])\n",
    "\n",
    "    inverse_transform_data = Compose([\n",
    "        T.MinMaxNormalize(T.log_transform(ctx['data_min'], k=k), \n",
    "                            T.log_transform(ctx['data_max'], k=k)).inverse_transform,\n",
    "        T.LogTransform(k=k).inverse_transform\n",
    "    ])\n",
    "\n",
    "    min_log = T.log_transform(ctx['data_min'], k)\n",
    "    max_log = T.log_transform(ctx['data_max'],k)\n",
    "    if return_ctx:\n",
    "        return  transform_data, transform_label, ctx\n",
    "    return  transform_data, transform_label, min_log, max_log\n",
    "\n",
    "def inverse_transform_auto_linear(dataset, amp_pred):\n",
    "    ### inverse transform autolinear predictions to original space\n",
    "    transform_data, transform_label, min_log, max_log = get_transforms_auto_linear(dataset, return_ctx=False)\n",
    "    amp_pred_np = T.minmax_denormalize(permute_fn(amp_pred).detach().cpu().numpy(), min_log,max_log)\n",
    "    amp_pred_np = T.log_inverse_transform(amp_pred_np, k=k)\n",
    "    return amp_pred_np\n",
    "\n",
    "def get_dataloader(dataset, transform_mode=\"normal\"):\n",
    "        \n",
    "    if transform_mode ==\"normal\":    \n",
    "        transform_data, transform_label, ctx = get_transforms(dataset, return_ctx=True)\n",
    "    \n",
    "    if transform_mode ==\"AutoLinear\":\n",
    "        transform_data, transform_label, ctx = get_transforms_auto_linear(dataset, return_ctx=True)\n",
    "\n",
    "\n",
    "    train_anno, val_anno = get_dataset_path(dataset)\n",
    "        \n",
    "    print(f'Loading {dataset} validation data')\n",
    "    dataset_valid = FWIDataset(\n",
    "        val_anno,\n",
    "        preload=True,\n",
    "        sample_ratio=sample_temporal,\n",
    "        file_size=ctx['file_size'],\n",
    "        transform_data=transform_data,\n",
    "        transform_label=transform_label\n",
    "    )\n",
    "        \n",
    "    valid_sampler = RandomSampler(dataset_valid)\n",
    "\n",
    "    dataloader_valid = DataLoader(\n",
    "                                dataset_valid, batch_size=batch_size,\n",
    "                                sampler=valid_sampler, num_workers=workers,\n",
    "                                pin_memory=True, collate_fn=default_collate, shuffle=False)\n",
    "    \n",
    "    print('Data loading over')\n",
    "        \n",
    "    return dataset_valid, dataloader_valid, transform_data, transform_label "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_forward_params(forward_model_params, model_path):\n",
    "        forward_model_params.setdefault('IUnetForwardModel', {})\n",
    "        forward_model_params['IUnetForwardModel']['cfg_path'] = cfg_path\n",
    "        forward_model_params['IUnetForwardModel']['latent_dim'] = latent_dim\n",
    "        \n",
    "        forward_model_params.setdefault('UNetForwardModel', {})\n",
    "        forward_model_params['UNetForwardModel']['cfg_path'] = cfg_path\n",
    "        forward_model_params['UNetForwardModel']['latent_dim'] = latent_dim\n",
    "        if \"UNetForwardModel_17M\" in model_path:\n",
    "            print(\"here\")\n",
    "            forward_model_params['UNetForwardModel']['unet_depth'] = architecture_params[\"UNetForwardModel_17M\"][\"unet_depth\"]\n",
    "            forward_model_params['UNetForwardModel']['unet_repeat_blocks'] = architecture_params[\"UNetForwardModel_17M\"][\"unet_repeat_blocks\"]\n",
    "        else:\n",
    "            forward_model_params['UNetForwardModel']['unet_depth'] = architecture_params[\"default\"][\"unet_depth\"]\n",
    "            forward_model_params['UNetForwardModel']['unet_repeat_blocks'] = architecture_params[\"default\"][\"unet_repeat_blocks\"]\n",
    "        forward_model_params['UNetForwardModel']['skip'] = skip # skip true\n",
    "        return forward_model_params\n",
    "\n",
    "def get_model(model_path, model_type):\n",
    "    try:\n",
    "        print(model_path)\n",
    "        forward_model_params = forward_network.forward_params\n",
    "        forward_model_params = set_forward_params(forward_model_params, model_path)\n",
    "        model = forward_network.model_dict[model_type](**forward_model_params[model_type]).to(device)\n",
    "        checkpoint = torch.load(model_path, map_location=torch.device('cpu'))\n",
    "        model.load_state_dict(checkpoint['model'])\n",
    "    except:\n",
    "        print(\"Failed to load new model. Falling back to Legacy Code.\")\n",
    "        forward_model_params = forward_network.forward_params_legacy\n",
    "        if \"UNetForwardModel_17M\" in model_path:\n",
    "            print(\"here\")\n",
    "            forward_model_params['unet_depth'] = architecture_params[\"UNetForwardModel_17M\"][\"unet_depth\"]\n",
    "            forward_model_params['unet_repeat_blocks'] = architecture_params[\"UNetForwardModel_17M\"][\"unet_repeat_blocks\"]\n",
    "        else:\n",
    "            forward_model_params['unet_depth'] = architecture_params[\"default\"][\"unet_depth\"]\n",
    "            forward_model_params['unet_repeat_blocks'] = architecture_params[\"default\"][\"unet_repeat_blocks\"]\n",
    "        model_type = model_type+\"_Legacy\"\n",
    "        model = forward_network.model_dict[model_type](**forward_model_params).to(device)\n",
    "        checkpoint = torch.load(model_path, map_location=torch.device('cpu'))\n",
    "        model.load_state_dict(checkpoint['model'])\n",
    "\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def get_model_iunet(model_path, model_type):\n",
    "\n",
    "    amp_input_channel = 5\n",
    "    amp_encoder_channel = [8, 16, 32, 64, 128]\n",
    "    amp_decoder_channel = [128, 64, 32, 16, 5]\n",
    "    amp_model = iunet_network.AmpAutoEncoder(amp_input_channel, amp_encoder_channel, amp_decoder_channel).to(device)\n",
    "\n",
    "    # creating velocity cnn\n",
    "    vel_input_channel = 1\n",
    "    vel_encoder_channel = [8, 16, 32, 64, 128]\n",
    "    vel_decoder_channel = [128, 64, 32, 16, 1]\n",
    "    vel_model = iunet_network.VelAutoEncoder(vel_input_channel, vel_encoder_channel, vel_decoder_channel).to(device)\n",
    "\n",
    "    if model_type == \"IUNET\":\n",
    "        iunet_model = iUNet(in_channels=128, dim=2, architecture=(4,4,4,4))\n",
    "        model = iunet_network.IUnetModel(amp_model, vel_model, iunet_model)\n",
    "        print(\"IUnet model initialized.\")\n",
    "    elif model_type == \"Decouple_IUnet\":\n",
    "        amp_iunet_model = iUNet(in_channels=128, dim=2, architecture=(4,4,4,4))\n",
    "        vel_iunet_model = iUNet(in_channels=128, dim=2, architecture=(4,4,4,4))\n",
    "        model = iunet_network.Decouple_IUnetModel(amp_model, vel_model, amp_iunet_model, vel_iunet_model)\n",
    "        print(\"Decoupled IUnetModel model initialized.\")\n",
    "    else:\n",
    "        print(f\"Invalid Model: {model_type}\")\n",
    "\n",
    "    checkpoint = torch.load(model_path, map_location=torch.device('cpu'))\n",
    "    model.load_state_dict(checkpoint['model'])\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    return model\n",
    "\n",
    "def get_model_auto_linear(model_path, model_type):\n",
    "    # Load the TorchScript model\n",
    "    model = torch.jit.load(model_path)\n",
    "\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def permute_fn(tensor):\n",
    "#     return torch.einsum(\"ijkl->iklj\", tensor)\n",
    "def permute_fn(tensor):\n",
    "    return tensor\n",
    "\n",
    "def plot_instance(amp_true, vel_true, transform_data, \n",
    "                  amp_true_auto_linear, vel_true_auto_linear, transform_data_auto_linear,\n",
    "                  model_paths, model_train_dataset=\"FlatVel-A\", evaluate_dataset=\"flatvel-b\", plot=True):\n",
    "    values_dict = {}\n",
    "    \n",
    "    amp_true_np = transform_data.inverse_transform(permute_fn(amp_true).detach().cpu().numpy())\n",
    "    values_dict[\"ground_truth\"] = amp_true_np\n",
    "    for i, architecture_name in enumerate(architecture_names):\n",
    "        print(\"Evaluating Architecture: \", architecture_name)\n",
    "\n",
    "        model_path = os.path.join(base_path, \n",
    "                                  model_paths[model_train_dataset][architecture_name], \n",
    "                                  \"latest_checkpoint.pth\")\n",
    "        print(\"Evaluating Model: \", model_path)\n",
    "        #load model\n",
    "        if architecture_types[i] == \"IUNET\":\n",
    "            model = get_model_iunet(model_path, architecture_types[i])\n",
    "            model = model.to(device)\n",
    "            amp_pred = model.forward(vel_true)\n",
    "        elif architecture_types[i] == \"AutoLinearForward\":\n",
    "            model = get_model_auto_linear(model_path, architecture_types[i])\n",
    "            model = model.to(device)\n",
    "            output = model(vel_true_auto_linear, amp_true_auto_linear)\n",
    "            amp_pred_auto_linear = output[4]\n",
    "        else:\n",
    "            model = get_model(model_path, architecture_types[i])  \n",
    "            model = model.to(device)\n",
    "            amp_pred = model(vel_true)\n",
    "            \n",
    "         ### diff normalization so diff inverse transofrm, not doing for inversion net as we retrain it with right normalization\n",
    "        if architecture_types[i] == \"AutoLinearForward\":\n",
    "            amp_pred_np = inverse_transform_auto_linear(evaluate_dataset, amp_pred_auto_linear)\n",
    "        else:\n",
    "            amp_pred_np = transform_data.inverse_transform(permute_fn(amp_pred).detach().cpu().numpy())\n",
    "        values_dict[architecture_name] = amp_pred_np\n",
    "    \n",
    "    ## viz path for vizualizing all models in one place\n",
    "    vis_path = os.path.join(base_path, model_train_dataset, viz_save_path)\n",
    "    if not os.path.exists(vis_path):\n",
    "        os.makedirs(vis_path) \n",
    "    ################\n",
    "    generate_plot_viz(values_dict, vis_path, evaluate_dataset, num_images, plot=plot)\n",
    "    generate_plot_trace(values_dict, os.path.join(vis_path, \"trace_plots_forward\", mode), evaluate_dataset, num_images, plot=plot)\n",
    "    return values_dict\n",
    "\n",
    "def generate_plot_viz(values_dict, vis_path, evaluate_dataset, num_images, plot=True):\n",
    "    save_name = f\"{evaluate_dataset}_{mode}_{num_images}\"\n",
    "    num_cols = len(values_dict.keys())\n",
    "    \n",
    "#     fig, axes = plt.subplots(num_images, num_cols, figsize=(3.5*num_cols, int(3*num_images)), dpi=150)\n",
    "    fig = plt.figure(figsize=(3.3*num_cols, int(3.3*num_images)), dpi=150)\n",
    "    gs = gridspec.GridSpec(num_images, num_cols + 1, width_ratios=[1]*num_cols + [0.1])\n",
    "    for i in range(num_images):           \n",
    "        amp_index=np.random.choice(5, 1)[0]\n",
    "        for j, (key, values) in enumerate(values_dict.items()):\n",
    "#             ax = axes[i, j]\n",
    "            ax = fig.add_subplot(gs[i, j])\n",
    "            img = ax.imshow(values_dict[key][i, 0], aspect='auto', vmin=-1, vmax=1, cmap=\"seismic\")\n",
    "#             divider = make_axes_locatable(ax)\n",
    "#             cax = divider.append_axes(\"right\", size=\"10%\", pad=0.05)\n",
    "#             cbar = ax.figure.colorbar(img,cax)\n",
    "#             cbar.ax.tick_params(labelsize=font_sizes[\"color_bar\"])  \n",
    "#             plt.colorbar(img, cax=cax)\n",
    "            plot_name = plot_names[key] if key in plot_names else key\n",
    "            ax.set_title(f\"{plot_names[key]}\", fontsize=font_sizes[\"sub_plt_title\"])\n",
    "#             ax.set_title(f\"{key}: Image {i}\", fontsize=12)\n",
    "            if j==0:\n",
    "                ax.set_ylabel(plot_dataset_names[evaluate_dataset], fontsize=font_sizes[\"dataset_name\"])\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([]) \n",
    "        divider = make_axes_locatable(ax)\n",
    "        cax = fig.add_subplot(gs[i, num_cols])\n",
    "        cbar = fig.colorbar(img, cax=cax)\n",
    "        cbar.ax.tick_params(labelsize=font_sizes[\"color_bar\"]) \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(vis_path, f\"{save_name}_forward.pdf\"))\n",
    "    if plot:\n",
    "        plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "\n",
    "def plot_trace_plots(values_dict, indices, direction='horizontal', image_id=0, vis_path=\"\", save_name=\"\", plot=True):\n",
    "    \"\"\"\n",
    "    Plot trace plots for the given indices.\n",
    "\n",
    "    Parameters:\n",
    "    values_dict (dictionary): The ground truth matrix with shape (h, w).\n",
    "    indices (list of int): The indices at which to plot the trace plots.\n",
    "    direction (str): The direction of the trace plot ('horizontal' or 'vertical').\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # Determine the number of plots\n",
    "    num_trace_plots = len(indices)\n",
    "    num_comparison_plots = len(values_dict)\n",
    "    \n",
    "    # Plot the original u_gt, u_pred and the difference\n",
    "#     fig, axes = plt.subplots(1, num_comparison_plots, figsize=(3*num_comparison_plots, 2.7))\n",
    "    fig = plt.figure(figsize=(3*num_comparison_plots, 3))\n",
    "    gs = gridspec.GridSpec(1, num_comparison_plots + 1, width_ratios=[1]*num_comparison_plots + [0.1])\n",
    "    for i, (key, values) in enumerate(values_dict.items()):\n",
    "#         ax = axes[i]\n",
    "        ax = fig.add_subplot(gs[i])\n",
    "        im = ax.imshow(values[image_id, 0], aspect='auto', vmin=-1, vmax=1, cmap=\"seismic\")\n",
    "        ax.set_title(f'{plot_names[key]}')\n",
    "        for idx in indices:\n",
    "            ax.axhline(y=idx, color='k', linestyle='--') if direction == 'horizontal' else ax.axvline(x=idx, color='k', linestyle='--')\n",
    "        xlabels = \"Sensors Locations (m)\"\n",
    "        ax.set_xlabel(xlabels)\n",
    "        ylabels = \"Time (ms)\"\n",
    "        if i==0:\n",
    "            ax.set_ylabel(ylabels)\n",
    "#     plt.colorbar(im, ax=ax)\n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = fig.add_subplot(gs[num_comparison_plots])\n",
    "    cbar = fig.colorbar(im, cax=cax)\n",
    "    cbar.ax.tick_params(labelsize=font_sizes[\"color_bar\"]) \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(vis_path, f\"{save_name}_Image{image_id}_{direction}_compare.pdf\"))\n",
    "    if plot:\n",
    "        plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "    # Set up the figure with subplots in a N x 1 configuration\n",
    "    fig, axes = plt.subplots(1, num_trace_plots, figsize=(3 * num_trace_plots, 2.7))\n",
    "    \n",
    "    # If there's only one index, axes will not be an array, so we wrap it in a list for consistency\n",
    "    if num_trace_plots == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    # Loop over the indices and plot the trace plots\n",
    "    for i, idx in enumerate(indices):\n",
    "        for j, (key, values) in enumerate(values_dict.items()):\n",
    "            slice_ = values[image_id, 0, idx, :] if direction == 'horizontal' else values[image_id, 0, :, idx]\n",
    "            linestyle = plot_map[key][\"linestyle\"]\n",
    "            color = plot_map[key][\"color\"]\n",
    "            zorder = plot_map[key][\"zorder\"]\n",
    "            axes[i].plot(slice_, linestyle=linestyle, color=color, zorder=zorder, label=f'{plot_names[key]}')\n",
    "                \n",
    "        # Set labels and title\n",
    "        axes[i].set_ylabel(f\"Trace at index {idx} {direction}\")\n",
    "        xlabels = \"Sensors Locations\" if direction==\"horizontal\" else \"Time (ms)\"\n",
    "        axes[i].set_xlabel(xlabels)\n",
    "#         axes[i].legend(loc='upper right', fontsize=8)\n",
    "    \n",
    "    handles, labels = axes[0].get_legend_handles_labels()\n",
    "\n",
    "    fig.legend(handles, labels, bbox_to_anchor=(0.5, 0.05),loc='upper center', ncol=4)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(vis_path, f\"{save_name}_Image{image_id}_{direction}_TracePlot.pdf\"), \n",
    "                             bbox_inches=\"tight\")\n",
    "    if plot:\n",
    "        plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "def generate_plot_trace(values_dict, vis_path, evaluate_dataset, num_images, plot=True):\n",
    "    if not os.path.exists(vis_path):\n",
    "        os.makedirs(vis_path) \n",
    "    values_dict = {key: value for key, value in values_dict.items() if key in plot_map}\n",
    "    \n",
    "    save_name = evaluate_dataset\n",
    "    horizontal_indices = [249, 499, 749]\n",
    "    horizontal_indices = [150, 350, 550]\n",
    "    vertical_indices = [17, 34, 51]\n",
    "    for i in range(num_images):\n",
    "        plot_trace_plots(values_dict, horizontal_indices, direction='horizontal', image_id=i, vis_path=vis_path, save_name=save_name, plot=plot)\n",
    "        plot_trace_plots(values_dict, vertical_indices, direction='vertical', image_id=i, vis_path=vis_path, save_name=save_name, plot=plot)\n",
    "\n",
    "        \n",
    "def do_everything(model_train_datasets, evaluate_datasets, model_paths):\n",
    "    items_dict = {}\n",
    "    for idx, evaluate_dataset in enumerate(evaluate_datasets):\n",
    "        print(\"Target Dataset: \", evaluate_dataset)\n",
    "        dataset_val, _, transform_data, transform_label = get_dataloader(evaluate_dataset)\n",
    "        if num_images == 20:\n",
    "            items = true_items_dict[evaluate_dataset]\n",
    "        else:\n",
    "            items = np.random.choice(len(dataset_val), num_images)\n",
    "            items_dict[evaluate_dataset] = items\n",
    "\n",
    "        _, amp_true, vel_true = dataset_val[items]\n",
    "        amp_true, vel_true = torch.tensor(amp_true).to(device), torch.tensor(vel_true).to(device)\n",
    "        \n",
    "         #### diff normalization for auto linear\n",
    "        dataset_val_auto_linear, _, transform_data_auto_linear, transform_label_auto_linear = get_dataloader(evaluate_dataset, transform_mode = \"AutoLinear\")  \n",
    "        _, amp_true_auto_linear, vel_true_auto_linear = dataset_val_auto_linear[items]\n",
    "        amp_true_auto_linear, vel_true_auto_linear = torch.tensor(amp_true_auto_linear).to(device), torch.tensor(vel_true_auto_linear).to(device)\n",
    "        \n",
    "       \n",
    "        for model_train_dataset in [model_train_datasets[idx]]:\n",
    "            print(\"Source Dataset: \", model_train_dataset)\n",
    "            value_dict = plot_instance(amp_true, vel_true, transform_data, \n",
    "                                       amp_true_auto_linear, vel_true_auto_linear, transform_data_auto_linear,\n",
    "                                       model_paths, model_train_dataset, evaluate_dataset, plot=False)\n",
    "    return value_dict, items_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_dict=do_everything(model_train_datasets, evaluate_datasets, model_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openfwi_env",
   "language": "python",
   "name": "openfwi_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
